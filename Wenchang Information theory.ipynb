{
 "metadata": {
  "name": "Wenchang Information theory"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Information Theory 101\n",
      "Information Theory was invented by Claude Shannon in a remarkable 1948 paper entitled \"A Mathematical Theory of Communication\".\n",
      "Today its uses extend far beyond communication.  We are going to use it in several ways in this course, so we begin with an overview of the basic elements of the theory.\n",
      "\n",
      "##Entropy\n",
      "Suppose that $X$ is a discrete random variable taking values $x_i$ with probability $p_i$ for $1\\le i\\le n$.\n",
      "We want to measure the uncertainty in $X$, but it's not clear how this should be defined.  Not so obviously, we put\n",
      "$$\n",
      "H(X) = -\\sum_{i=1}^np_i\\log(p_i).\n",
      "$$\n",
      "where $0\\log(0) = \\lim_{p->0}p\\log(p) = 0$.  In other words, $H(X)$ is the expected value of $-\\log(p_i)$.  We call $H(X)$ the *entropy* of $X$.  The base of the logarithm determines the units of $H(X)$.  For base 2, the most common choice, the units are bits.  For base $e$, the units are \"nats\", and for base 10, the units are \"bans\".\n",
      "\n",
      "It's clear that entropy is always non-negative, and that it is zero precisely when $p_i = 1$ for some $i$ and $p_j = 0$ for all $j \\neq i$.  Of course, this is exactly the case where there is no uncertainty at all because we know in advance of any experiment that $X = x_i$. Furthermore, it's a fairly easy calculus exercise to see that the maximum possible value of $H(X)$ is $\\log(n)$, and that it occurs precisely when $p_i = 1/n$ for all $i$. This is clearly the case of maximum uncertainty, so again our definition behaves properly.\n",
      "\n",
      "Let's see how this works in practice.  We'll take $n = 16$ and generate a random discrete distribution as an ndarray $p$ of length 16 which will define a random variable $X$ taking values $\\{1,2,\\dots,16\\}$, and we'll then compute its entropy:    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math as m\n",
      "import numpy as np\n",
      "rv = np.random.sample(26)  #corresponding to 26 letters\n",
      "dis = rv/rv.sum() \n",
      "H = -sum([a*m.log(b) for a, b in zip(dis,dis)])\n",
      "print H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.12980012549\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Huffman Trees\n",
      "So far, so good, but there are infintely many different possible functions $H(X)$ with the above two properties.\n",
      "Here is some intuition about why our definition is what it is.\n",
      "\n",
      "Suppose you have sampled $X$ and you have a value $x_i$.  I want to find out which $x$ you have by means of a sequence of yes-no questions. Each question can depend on the answers to the previous questions, just like the game \"twenty questions\".  So the sequence of questions forms a path through a binary tree which ends at a leaf when all values but one have been eliminated.  \n",
      "\n",
      "It's pretty clear that the optimal way to design the tree is to choose a subset $S$ of values whose total probability is $1/2$, or as close to $1/2$ as possible, and ask the question \"Is $x$ in $S$?\"\n",
      "The answer will eliminate either $S$ or its complement, and then we can renormalize the remaining probabilities to\n",
      "sum to one and recurse.  Intuitively, the answer to each question reduces the uncertainty in $x$ by one bit.  We usually say that each question provides one bit of information about $x$.  This is actually the definition of information:  An experiment yields $b$ bits of information about $X$ when it's outcome reduces the uncertainty in $X$ by $b$ bits.\n",
      "\n",
      "Now we can see where our formula for $H(X)$ comes from.  For the sake of simplicity, lets assume that at each step, it's possible to choose $S$ with total probability exactly $1/2$.  Since $x_i$ is never eliminated (because you aren't allowed to lie) its probability is doubled at each recursive step because we divide all remaining probabilities by $1/2$ at each step. So we reach the leaf $x = x_i$ precisely when the renormalized probability of $x_i$ reaches 1.  Solving the equation $2^kp_i = 1$, we see that it takes (approximately) $-\\log_2(p_i)$ questions to determine that $x = x_i$.  So $H(X)$ is the expected number of (optimal) questions needed to determine a random value of $X$.\n",
      "\n",
      "##Data Compression\n",
      "There's another use for the above tree which explains what the whole subject has to do with communications. Suppose that the values $x_i$ of $X$ are symbols which we concatenate to form a text of some length, say $N$.  For example, with $n = 16$ they might be the 16 different nibbles. Further, suppose that $p_i$ is the probability of occurence of $x_i$ in our text. If we replace the branch labels \"yes\" and \"no\" with $1$ and $0$ respectively, the path from the root to $x_i$ defines a binary codeword $c_i$ of length $-\\log_2{p_i}$ bits (in the ideal case).  So we can replace each occurence of $x_i$ in our text with $c_i$, and you can see (exercise!) that instead of the original $4N$ bits needed for our text of $N$ nibbles, we have compressed the text to roughly $H(X)N$ bits.  Furthermore, it is clear that this code is *prefix-free*, which means that no codeword is a prefix of any other codeword.  This means that we can decode a string of concatenated codewords without needing any special stop symbol to signal the end of the current codeword. Shannon was originally interested in data compression (which he called \"source coding\") to improve the efficiency of digital communications.\n",
      "\n",
      "##Huffman's Algorithm\n",
      "How do we actually build the tree?  At first glance, it looks like in order to take the first step, we have to inspect all possible subsets $S$ in order to find one whose total probability is as close to $1/2$ as possible.  But this procedure has exponential work and is completely impractical for any reasonably large value $n$, like $n=256$ for example.  Fortunately, a graduate student at MIT by the name of David Huffman discovered an efficient algorithm which has complexity $O(n\\log{n})$.  The trick is to build the tree from the bottom up, rather than from the top down.  First, sort the $p_i$ so that $p_1\\le p_2\\le \\dots \\le p_n$.  Let $y_1$ be the parent node of $x_1$. Now a moment's thought will convince you that we must have $y_1 = ${$x_1,x_2$}  because this makes $p_1$ as close to \n",
      "$Pr(y_1)/2$ as possible.  Now let $X^{(1)}$ be a new random variable taking values $y_1,x_3,\\dots,x_n$ with probabilities\n",
      "$p_1+p_2,p_3,\\dots,p_n$.  Re-sort the probabilities (note: work is $O(\\log{n})$ to insert $p_1+p_2$ into the sorted list\n",
      "$p_3,\\dots,p_n$) and recurse.  It's pretty clear that this algorithm produces optimal trees, which is why they are called Huffman trees.\n",
      "\n",
      "Let's see how this works by writing a program to find an optimal tree for the random variable $X$ that we generated above, and compute the average distance of each leaf from the root: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import itemgetter, attrgetter\n",
      "from sets import Set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class tree(object):\n",
      "    def __init__(self, data = None, left = None, right = None):\n",
      "        self.data = data\n",
      "        self.left = left\n",
      "        self.right = right\n",
      "\n",
      "class event(object):\n",
      "    def __init__(self, sets = None, prob = 0):\n",
      "        self.Set = sets\n",
      "        self.Prob = prob\n",
      "\n",
      "# Build Huffman Tree\n",
      "def BuildHTree(dis):\n",
      "    #To store tree node we have built without tree structure\n",
      "    treeList = [] \n",
      "    \n",
      "    #list of pairs of symbol and its probability for sake of sorting by probability\n",
      "    EventSeq = [] \n",
      "    for i in range(0, len(dis)):\n",
      "        EventSeq.append(event(set([i]), dis[i]))\n",
      "    \n",
      "    while len(EventSeq) > 1:\n",
      "        EventSeq.sort(key = attrgetter('Prob'))\n",
      "        Event1 = EventSeq[0]\n",
      "        Event2 = EventSeq[1]\n",
      "        \n",
      "        EventNew = event(sets = Event1.Set.union(Event2.Set), prob = Event1.Prob + Event2.Prob)\n",
      "        #construct new tree node\n",
      "        TreeNew = tree(data = EventNew.Set)\n",
      "        \n",
      "        #left\n",
      "        if len(Event1.Set) > 1:\n",
      "            indL = 0\n",
      "            while treeList[indL].data != Event1.Set:\n",
      "                indL = indL + 1\n",
      "            TreeNew.left = treeList[indL]\n",
      "        else:\n",
      "            TreeNew.left = Event1.Set\n",
      "        \n",
      "        #right\n",
      "        if len(Event2.Set) > 1:\n",
      "            indR = 0\n",
      "            while treeList[indR].data != Event2.Set:\n",
      "                indR = indR + 1\n",
      "            TreeNew.right = treeList[indR]\n",
      "        else:\n",
      "            TreeNew.right = Event2.Set\n",
      "        \n",
      "        treeList.append(TreeNew)\n",
      "        \n",
      "        # update events sequence\n",
      "        EventSeq.append(EventNew)\n",
      "        EventSeq = EventSeq[2:]\n",
      "    return treeList[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTree1 = BuildHTree(dis)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using above tree to encode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# left = 1 and right = 0\n",
      "def HTreeCodeDict(HTree):\n",
      "    Num = len(HTree.data)\n",
      "    symbol = list(HTree.data)\n",
      "    LstCode = dict()\n",
      "    for i in symbol:\n",
      "        LstCode[i] = []\n",
      "        tree = HTree # start search from the head of the tree\n",
      "        Set = set([i])\n",
      "        while type(tree) != set:\n",
      "            if type(tree.left) == set and Set == tree.left:\n",
      "                LstCode[i].append(1)\n",
      "                tree = tree.left\n",
      "            elif type(tree.left) != set and Set.issubset(tree.left.data):\n",
      "                LstCode[i].append(1)\n",
      "                tree = tree.left\n",
      "            elif type(tree.right) == set and Set == tree.right:\n",
      "                LstCode[i].append(0)\n",
      "                tree = tree.right\n",
      "            elif type(tree.right) != set and Set.issubset(tree.right.data):\n",
      "                LstCode[i].append(0)\n",
      "                tree = tree.right\n",
      "    return LstCode"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LstCode1 = HTreeCodeDict(HTree1)\n",
      "LstCode1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{0: [0, 1, 1, 1, 1],\n",
        " 1: [1, 0, 1, 1, 1, 0],\n",
        " 2: [0, 0, 1, 0, 0, 0, 0],\n",
        " 3: [0, 1, 0, 0],\n",
        " 4: [0, 0, 0, 0, 1],\n",
        " 5: [0, 0, 1, 0, 0, 0, 1],\n",
        " 6: [0, 0, 0, 1, 1],\n",
        " 7: [0, 0, 0, 0, 0, 1],\n",
        " 8: [1, 0, 1, 1, 1, 1],\n",
        " 9: [1, 1, 0, 1, 1],\n",
        " 10: [1, 0, 0, 0],\n",
        " 11: [0, 1, 0, 1],\n",
        " 12: [0, 0, 1, 0, 0, 1],\n",
        " 13: [0, 0, 0, 0, 0, 0],\n",
        " 14: [1, 0, 1, 0],\n",
        " 15: [1, 0, 1, 1, 0],\n",
        " 16: [1, 1, 1, 0],\n",
        " 17: [1, 1, 1, 1],\n",
        " 18: [1, 1, 0, 0],\n",
        " 19: [0, 0, 1, 0, 1],\n",
        " 20: [0, 0, 0, 1, 0],\n",
        " 21: [1, 1, 0, 1, 0],\n",
        " 22: [0, 0, 1, 1],\n",
        " 23: [1, 0, 0, 1],\n",
        " 24: [0, 1, 1, 0],\n",
        " 25: [0, 1, 1, 1, 0]}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def HTreeEncode(SymArry, HTree):\n",
      "    CodeDict = HTreeCodeDict(HTree)\n",
      "    CodeLst = []\n",
      "    for sym in SymArry:\n",
      "        CodeLst = CodeLst + CodeDict[sym]\n",
      "    return CodeLst\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CodeLst = HTreeEncode([5,1,0,6,1,0,5,7,1,6], HTree1)\n",
      "CodeLst"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using the same tree to decode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def HTreeDecode(CodeLst, HTree):\n",
      "    SymbolArry = []\n",
      "    tree = HTree\n",
      "    i = 0\n",
      "    while i < len(CodeLst):\n",
      "        if CodeLst[i]:\n",
      "            tree = tree.left\n",
      "            i = i + 1\n",
      "            if type(tree) == set:\n",
      "                SymbolArry.append(list(tree)[0])\n",
      "                tree = HTree\n",
      "        else:\n",
      "            tree = tree.right\n",
      "            i = i + 1\n",
      "            if type(tree) == set:\n",
      "                SymbolArry.append(list(tree)[0])\n",
      "                tree = HTree\n",
      "    return SymbolArry"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Resym = HTreeDecode(CodeLst, HTree1)\n",
      "Resym"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[5, 1, 0, 6, 1, 0, 5, 7, 1, 6]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Conditional Entropy and Mutual Information\n",
      "In addition to data compression and many other uses in communication theory, information theory turns out to be very useful for studying dependencies between pairs of random variables, and this will be our actual application.\n",
      "\n",
      "So suppose we have another random variable $Y$ taking values $y_j$ with probability $q_j$ for $1\\le j\\le m$.\n",
      "Then there is a joint distribution $r_{ij} := Pr(X = x_i \\& Y = y_j)$ with marginals\n",
      "\n",
      "$$ \n",
      "p_i = \\sum_jr_{ij}\\quad(1\\le i\\le n) \n",
      "$$\n",
      "and\n",
      "$$\n",
      "q_j = \\sum_ir_{ij}\\quad(1\\le j\\le m).\n",
      "$$\n",
      "The *joint entropy* is just the entropy of the joint distribution:\n",
      "\n",
      "$$\n",
      "H(X,Y) := -\\sum_{i,j}r_{ij}\\log(r_{ij}).\n",
      "$$\n",
      "Given a value $Y = y_j$, the *conditional entropy* is just the entropy of the conditional distribution:\n",
      "\n",
      "$$\n",
      "H(X\\mid y_j) = -\\sum_i\\frac{r_{ij}}{q_j}\\log(\\frac{r_{ij}}{q_j}),\n",
      "$$ \n",
      "and we define\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) := \\sum_jq_jH(X\\mid y_j) = -\\sum_{ij}r_{ij}(\\log{r_{ij}} - \\log{q_j})\n",
      "= H(X,Y) + \\sum_j q_j\\log{q_j}\n",
      "= H(X,Y) - H(Y).\n",
      "$$\n",
      "\n",
      "This is a very nice result, for two reasons.  First, since entropy is always non-negative, we see that $H(X,Y) \\ge H(Y)$.\n",
      "More importantly, by symmetry we get\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) + H(Y) = H(X,Y) = H(Y\\mid X) + H(X)\n",
      "$$\n",
      "\n",
      "so that we can define\n",
      "\n",
      "$$\n",
      "I(X,Y) := H(X) - H(X\\mid Y) = H(Y) - H(Y\\mid X) = H(X) + H(Y) -H(X,Y).\n",
      "$$\n",
      "\n",
      "So $I(X,Y)$ is the expected reduction in the entropy of $X$ given a random sample from $Y$, or in other words it is the information about $X$ obtained by knowing $Y$. By symmetry, it is also the information about $Y$ obtained by knowing $X$, so we call $I(X,Y)$ the *mutual information* of $X$ and $Y$.  A simple calculation (exercise!) shows that\n",
      "$$\n",
      "I(X,Y) = \\sum_{ij}r_{ij}\\log(\\frac{r_{ij}}{p_iq_j}).\n",
      "$$\n",
      "\n",
      "In particular, $I(X,Y) \\ge 0$, and it's not hard to see (exercise!) that equality holds iff $r_{ij} = p_iq_j$ for all $i,j$. In other words, the mutual information of two random variables is zero precisely when they are independent.  More generally, $I(X,Y)$ is a measure of the dependence between $X$ and $Y$. How big can it get?  From the above equations it follows easily that $I(X,Y)$ is maximized for a fixed $X$ precisely when $Y = X$ (exercise!).\n",
      "\n",
      "To illustrate these ideas, we will generate a 16x10 ndarray $r$ of non-negative reals such that the $i^{th}$ row sum is $p_i$ for $1\\le i\\le 16$.  Then we'll call the $j^{th}$ column sum $q_j$ (and show that the $q_j$ sum to 1), giving us another random variable $Y$ taking values $\\{1,2,\\dots,10\\}$.  Then we'll compute $H(Y), H(Y\\mid X), H(X\\mid Y), H(X,Y)$, and $I(X,Y)$, and verify the above formulas: "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}