{
 "metadata": {
  "name": "Course_Notes_scratch"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Information Theory 101\n",
      "Information Theory was invented by Claude Shannon in a remarkable 1948 paper entitled \"A Mathematical Theory of Communication\".\n",
      "Today its uses extend far beyond communication.  We are going to use it in several ways in this course, so we begin with an overview of the basic elements of the theory.\n",
      "\n",
      "##Entropy\n",
      "Suppose that $X$ is a discrete random variable taking values $x_i$ with probability $p_i$ for $1\\le i\\le n$.\n",
      "We want to measure the uncertainty in $X$, but it's not clear how this should be defined.  Not so obviously, we put\n",
      "$$\n",
      "H(X) = -\\sum_{i=1}^np_i\\log(p_i).\n",
      "$$\n",
      "where $0\\log(0) = \\lim_{p->0}p\\log(p) = 0$.  In other words, $H(X)$ is the expected value of $-\\log(p_i)$.  We call $H(X)$ the *entropy* of $X$.  The base of the logarithm determines the units of $H(X)$.  For base 2, the most common choice, the units are bits.  For base $e$, the units are \"nats\", and for base 10, the units are \"bans\".\n",
      "\n",
      "It's clear that entropy is always non-negative, and that it is zero precisely when $p_i = 1$ for some $i$ and $p_j = 0$ for all $j \\neq i$.  Of course, this is exactly the case where there is no uncertainty at all because we know in advance of any experiment that $X = x_i$. Furthermore, it's a fairly easy calculus exercise to see that the maximum possible value of $H(X)$ is $\\log(n)$, and that it occurs precisely when $p_i = 1/n$ for all $i$. This is clearly the case of maximum uncertainty, so again our definition behaves properly.\n",
      "\n",
      "Let's see how this works in practice.  We'll take $n = 16$ and generate a random discrete distribution as an ndarray $p$ of length 16 which will define a random variable $X$ taking values $\\{1,2,\\dots,16\\}$, and we'll then compute its entropy:    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "from math import log\n",
      "\n",
      "random.seed(1234)\n",
      "m = 16\n",
      "\n",
      "p = [random.random() for i in range(0,m)]\n",
      "s = sum(p)\n",
      "p = [p[i]/s for i in range(0,m)]\n",
      "\n",
      "HX = -sum(q*log(q) for q in p)\n",
      "print \"p: \",p\n",
      "print \"Entropy of p: \",HX/log(2),\" bits per symbol\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "p:  [0.11844852881133704, 0.054016179819892249, 0.00091815444230160877, 0.11164919838315231, 0.11511679225857523, 0.071357801399979645, 0.082306808809699969, 0.010287467646092813, 0.093939890018727212, 0.029023402019856322, 0.0037765659759976125, 0.096671970739129337, 0.042416657706630931, 0.076389366924792257, 0.075474361063530462, 0.01820685398030511]\n",
        "Entropy of p:  3.63807761268  bits per symbol\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Huffman Trees\n",
      "So far, so good, but there are infintely many different possible functions $H(X)$ with the above two properties.\n",
      "Here is some intuition about why our definition is what it is.\n",
      "\n",
      "Suppose you have sampled $X$ and you have a value $x_i$.  I want to find out which $x$ you have by means of a sequence of yes-no questions. Each question can depend on the answers to the previous questions, just like the game \"twenty questions\".  So the sequence of questions forms a path through a binary tree which ends at a leaf when all values but one have been eliminated.  \n",
      "\n",
      "It's pretty clear that the optimal way to design the tree is to choose a subset $S$ of values whose total probability is $1/2$, or as close to $1/2$ as possible, and ask the question \"Is $x$ in $S$?\"\n",
      "The answer will eliminate either $S$ or its complement, and then we can renormalize the remaining probabilities to\n",
      "sum to one and recurse.  Intuitively, the answer to each question reduces the uncertainty in $x$ by one bit.  We usually say that each question provides one bit of information about $x$.  This is actually the definition of information:  An experiment yields $b$ bits of information about $X$ when it's outcome reduces the uncertainty in $X$ by $b$ bits.\n",
      "\n",
      "Now we can see where our formula for $H(X)$ comes from.  For the sake of simplicity, lets assume that at each step, it's possible to choose $S$ with total probability exactly $1/2$.  Since $x_i$ is never eliminated (because you aren't allowed to lie) its probability is doubled at each recursive step because we divide all remaining probabilities by $1/2$ at each step. So we reach the leaf $x = x_i$ precisely when the renormalized probability of $x_i$ reaches 1.  Solving the equation $2^kp_i = 1$, we see that it takes (approximately) $-\\log_2(p_i)$ questions to determine that $x = x_i$.  So $H(X)$ is the expected number of (optimal) questions needed to determine one random value of $X$. Of course, if we have a sequence of $N$ random values of $X$, we expect to ask $H(X)N$ questions to learn all\n",
      "the values.  So entropy is often thought of as a rate, in bits (or questions) per value of $X$. \n",
      "\n",
      "##Data Compression\n",
      "There's another use for the above tree which explains what the whole subject has to do with communications. Suppose that the values $x_i$ of $X$ are symbols which we concatenate to form a text of some length, say $N$.  For example, with $n = 16$ they might be the 16 different nibbles. Further, suppose that $p_i$ is the probability of occurence of $x_i$ in our text. If we replace the branch labels \"yes\" and \"no\" with $1$ and $0$ respectively, the path from the root to $x_i$ defines a binary codeword $c_i$ of length $-\\log_2{p_i}$ bits (in the ideal case).  So we can replace each occurence of $x_i$ in our text with $c_i$, and you can see (exercise!) that instead of the original $4N$ bits needed for our text of $N$ nibbles, we have compressed the text to roughly $H(X)N$ bits.  Furthermore, it is clear that this code is *prefix-free*, which means that no codeword is a prefix of any other codeword.  This means that we can decode a string of concatenated codewords without needing any special stop symbol to signal the end of the current codeword. Shannon was originally interested in data compression (which he called \"source coding\") to improve the efficiency of digital communications.\n",
      "\n",
      "##Huffman's Algorithm\n",
      "How do we actually build the tree?  At first glance, it looks like in order to take the first step, we have to inspect all possible subsets $S$ in order to find one whose total probability is as close to $1/2$ as possible.  But this procedure has exponential work and is completely impractical for any reasonably large value of $n$, like $n=256$ for example.  Fortunately, a graduate student at MIT by the name of David Huffman discovered an efficient algorithm which has complexity $O(n\\log{n})$.  The trick is to build the tree recursively from the bottom up, rather than from the top down.  First, sort the $p_i$ so that $p_1\\le p_2\\le \\dots \\le p_n$.  Let $y_1$ be the parent node of $x_1$. Now a moment's thought will convince you that we must have $y_1 = ${$x_1,x_2$}  because this makes $p_1$ as close to \n",
      "$Pr(y_1)/2$ as possible.  Now let $X^{(1)}$ be a new random variable taking values $y_1,x_3,\\dots,x_n$ with probabilities $p_1+p_2,p_3,\\dots,p_n$.  Re-sort the probabilities (note: work is $O(\\log{n})$ to insert $p_1+p_2$ into the sorted list\n",
      "$p_3,\\dots,p_n$) and recurse.  It's pretty clear that this algorithm produces optimal trees, which is why they are called Huffman trees.\n",
      "\n",
      "Let's see how this works by writing a program to find an optimal tree for the random variable $X$ that we generated above, and compute the average distance of each leaf from the root: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import copy\n",
      "from math import log\n",
      "class Tree:\n",
      "    def __init__(self,left,right,data):\n",
      "        self.left = left # left and right are subtrees for nodes and None for leaves\n",
      "        self.right = right\n",
      "        self.data = data # data is a set of symbols\n",
      "\n",
      "def my_key(x): # needed to sort pairs by first element\n",
      "    return x[0]\n",
      "class Huffman:\n",
      "    node = [] # where the nodes are stored\n",
      "    root = Tree(None,None,None)\n",
      "    def __init__(self,p):\n",
      "        n = len(p)\n",
      "        leaf = [[p[i], Tree(None,None,set([i]))] for i in range(0,n)] # initialize the leaves\n",
      "        node = copy.deepcopy(leaf) # initialize node list with the leaves\n",
      "        leaf.sort(key=my_key) # sort the leaves by their probability\n",
      "        new = []\n",
      "        \n",
      "        for i in range(0,n-1): # link the two remaining least likely \"leaves\" into a new node\n",
      "            left = leaf[i]\n",
      "            right = leaf[i+1]\n",
      "            new = [left[0]+right[0],Tree(left[1],right[1],left[1].data | right[1].data)]\n",
      "            node.append(new) # copy it into the node list\n",
      "            j = i+1\n",
      "            while j < n-1 and new[0] > leaf[j+1][0]: \n",
      "                leaf[j] = copy.deepcopy(leaf[j+1]) # move existing less likely \"leaves\" downward\n",
      "                j += 1\n",
      "            leaf[j] = new #insert the new node as a recursive \"leaf\"\n",
      "        self.node = node # save the node list and the root of the tree\n",
      "        self.root = new[1]\n",
      "        \n",
      "    def encode(self,i): # encode symbol i\n",
      "        node = self.root\n",
      "        code = \"\"\n",
      "        while node.left != None:\n",
      "            if i in node.left.data:\n",
      "                node = node.left\n",
      "                code += '0'\n",
      "            else:\n",
      "                node = node.right\n",
      "                code += \"1\"\n",
      "        return code\n",
      "    \n",
      "    def decode(self,s): # decode word s\n",
      "        node = self.root\n",
      "        for i in range(0,len(s)):\n",
      "            if s[i] == '0':\n",
      "                node = node.left\n",
      "            else:\n",
      "                node = node.right\n",
      "        return node.data\n",
      "                   \n",
      "Huff = Huffman(p)\n",
      "print \"Code Table:\\n\"\n",
      "H = 0\n",
      "for i in range(0,len(p)):\n",
      "    s = Huff.encode(i)\n",
      "    ii = Huff.decode(s)\n",
      "    print i,\" code: \",s,\" decode: \",ii,\" p: \",p[i],\" -log(p): \",-log(p[i])/log(2)\n",
      "    H += len(s)*p[i]\n",
      "print \"\\nCoding entropy: \",H,\" bits per symbol\"\n",
      "  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Code Table:\n",
        "\n",
        "0  code:  100  decode:  set([0])  p:  0.118448528811  -log(p):  3.07766781518\n",
        "1  code:  0001  decode:  set([1])  p:  0.0540161798199  -log(p):  4.21046457788\n",
        "2  code:  10101000  decode:  set([2])  p:  0.000918154442302  -log(p):  10.0889755305\n",
        "3  code:  010  decode:  set([3])  p:  0.111649198383  -log(p):  3.16295520192\n",
        "4  code:  011  decode:  set([4])  p:  0.115116792259  -log(p):  3.11882979798\n",
        "5  code:  1011  decode:  set([5])  p:  0.0713578014  -log(p):  3.8087850246\n",
        "6  code:  1110  decode:  set([6])  p:  0.0823068088097  -log(p):  3.60284440761\n",
        "7  code:  1010101  decode:  set([7])  p:  0.0102874676461  -log(p):  6.60296829637\n",
        "8  code:  1111  decode:  set([8])  p:  0.0939398900187  -log(p):  3.41211828523\n",
        "9  code:  10100  decode:  set([9])  p:  0.0290234020199  -log(p):  5.10663955288\n",
        "10  code:  10101001  decode:  set([10])  p:  0.003776565976  -log(p):  8.04870929391\n",
        "11  code:  001  decode:  set([11])  p:  0.0966719707391  -log(p):  3.37075853727\n",
        "12  code:  0000  decode:  set([12])  p:  0.0424166577066  -log(p):  4.55922524406\n",
        "13  code:  1101  decode:  set([13])  p:  0.0763893669248  -log(p):  3.71048435456\n",
        "14  code:  1100  decode:  set([14])  p:  0.0754743610635  -log(p):  3.72786955128\n",
        "15  code:  101011  decode:  set([15])  p:  0.0182068539803  -log(p):  5.77937453375\n",
        "\n",
        "Coding entropy:  3.6731919044  bits per symbol\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Conditional Entropy and Mutual Information\n",
      "In addition to data compression and many other uses in communication theory, information theory turns out to be very useful for studying dependencies between pairs of random variables, and this will be our actual application.\n",
      "\n",
      "So suppose we have another random variable $Y$ taking values $y_j$ with probability $q_j$ for $1\\le j\\le m$.\n",
      "Then there is a joint distribution $r_{ij} := Pr(X = x_i \\& Y = y_j)$ with marginals\n",
      "\n",
      "$$ \n",
      "p_i = \\sum_jr_{ij}\\quad(1\\le i\\le n) \n",
      "$$\n",
      "and\n",
      "$$\n",
      "q_j = \\sum_ir_{ij}\\quad(1\\le j\\le m).\n",
      "$$\n",
      "The *joint entropy* is just the entropy of the joint distribution:\n",
      "\n",
      "$$\n",
      "H(X,Y) := -\\sum_{i,j}r_{ij}\\log(r_{ij}).\n",
      "$$\n",
      "Given a value $Y = y_j$, the *conditional entropy* is just the entropy of the conditional distribution:\n",
      "\n",
      "$$\n",
      "H(X\\mid y_j) = -\\sum_i\\frac{r_{ij}}{q_j}\\log(\\frac{r_{ij}}{q_j}),\n",
      "$$ \n",
      "and we define\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) := \\sum_jq_jH(X\\mid y_j) = -\\sum_{ij}r_{ij}(\\log{r_{ij}} - \\log{q_j})\n",
      "= H(X,Y) + \\sum_j q_j\\log{q_j}\n",
      "= H(X,Y) - H(Y).\n",
      "$$\n",
      "\n",
      "This is a very nice result, for two reasons.  First, since entropy is always non-negative, we see that $H(X,Y) \\ge H(Y)$.\n",
      "More importantly, by symmetry we get\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) + H(Y) = H(X,Y) = H(Y\\mid X) + H(X)\n",
      "$$\n",
      "\n",
      "so that we can define\n",
      "\n",
      "$$\n",
      "I(X,Y) := H(X) - H(X\\mid Y) = H(Y) - H(Y\\mid X) = H(X) + H(Y) - H(X,Y).\n",
      "$$\n",
      "\n",
      "So $I(X,Y)$ is the expected reduction in the entropy of $X$ given a random sample from $Y$, or as it's often loosely described, the information about $X$ obtained by knowing $Y$. By symmetry, it is also the information about $Y$ obtained by knowing $X$, so we call $I(X,Y)$ the *mutual information* of $X$ and $Y$.  A simple calculation (exercise!) shows that\n",
      "$$\n",
      "I(X,Y) = \\sum_{ij}r_{ij}\\log(\\frac{r_{ij}}{p_iq_j}).\n",
      "$$\n",
      "\n",
      "It can be shown that $I(X,Y) \\ge 0$.  (This is intuitively clear, because knowing a value of $Y$ can't possibly *increase* our uncertainty about $X$.  Nevertheless, the proof isn't completely trivial.)  It's not hard to see (exercise!) that equality holds iff $r_{ij} = p_iq_j$ for all $i,j$. In other words, the mutual information of two random variables is zero precisely when they are independent.  More generally, $I(X,Y)$ is a measure of the dependence between $X$ and $Y$. How big can it get?  From the above equations it follows easily that $I(X,Y)$ is maximized for a fixed $X$ precisely when $Y = X$ (exercise!).\n",
      "\n",
      "To illustrate these ideas, we will generate a 16x10 ndarray $r$ of non-negative reals such that the $i^{th}$ row sum is $p_i$ for $1\\le i\\le 16$.  Then we'll call the $j^{th}$ column sum $q_j$ (and show that the $q_j$ sum to 1), giving us another random variable $Y$ taking values $\\{1,2,\\dots,10\\}$.  Then we'll compute $H(Y), H(Y\\mid X), H(X\\mid Y), H(X,Y)$, and $I(X,Y)$, and verify the above formulas: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = 16\n",
      "n = 10\n",
      "r = [[random.random() for j in range(0,n)] for i in range(0,m)]\n",
      "for i in range(0,m):\n",
      "    f = p[i]/sum(r[i])\n",
      "    r[i] = [r[i][j]*f for j in range(0,n)] \n",
      "q = [sum(r[i][j] for i in range(0,m)) for j in range(0,n)]\n",
      "\n",
      "print \"sum(q): \",sum(q)\n",
      "HY = -sum(q[j]*log(q[j]) for j in range(0,10))\n",
      "HYx = [-sum(r[i][j]/p[i]*log(r[i][j]/p[i]) for j in range(0,n)) for i in range(0,m)]\n",
      "HYbarX = sum(HYx[i]*p[i] for i in range(0,m))\n",
      "HXy = [-sum(r[i][j]/q[j]*log(r[i][j]/q[j]) for i in range(0,m)) for j in range(0,n)]\n",
      "HXbarY = sum(HXy[j]*q[j] for j in range(0,n))\n",
      "HXY = -sum(sum(r[i][j]*log(r[i][j]) for i in range(0,m)) for j in range(0,n))\n",
      "IXY = sum(sum(r[i][j]*log(r[i][j]/(p[i]*q[j])) for i in range(0,m)) for j in range(0,n))\n",
      "print \"H(X): \",HX\n",
      "print \"H(Y): \",HY\n",
      "print \"H(X|y_j): \",HXy\n",
      "print \"H(Y|x_i): \",HYx\n",
      "print \"H(X|Y): \", HXbarY\n",
      "print \"H(Y|X): \", HYbarX\n",
      "print \"H(X,Y): \", HXY\n",
      "print \"I(X,Y): \", IXY\n",
      "print \"H(X)-H(X|Y): \", HX-HXbarY\n",
      "print \"H(Y)-H(Y|X): \", HY-HYbarX\n",
      "print \"H(X)+H(Y)-H(X,Y): \",HX+HY-HXY\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sum(q):  1.0\n",
        "H(X):  2.52172323989\n",
        "H(Y):  2.29368292785\n",
        "H(X|y_j):  [2.4427510219149453, 2.2358450364480711, 2.3460216951593029, 2.2163516578034788, 2.1163827673380675, 2.3817213388969853, 2.3871990939060694, 2.4619557570580248, 2.2202431242380523, 2.2444268355735719]\n",
        "H(Y|x_i):  [1.9384284102687208, 2.2342510102645994, 1.941362754576079, 1.9960381055355088, 2.0659687257455186, 2.1640684715711402, 2.2193644159677568, 2.0887624848154496, 1.9851410536508407, 2.0746777877254106, 2.2222381910357285, 2.0087338465030329, 2.174604446461029, 2.1804973392796292, 2.0842449781694787, 2.1746040786742187]\n",
        "H(X|Y):  2.30436112426\n",
        "H(Y|X):  2.07632081223\n",
        "H(X,Y):  4.59804405211\n",
        "I(X,Y):  0.217362115629\n",
        "H(X)-H(X|Y):  0.217362115629\n",
        "H(Y)-H(Y|X):  0.217362115629\n",
        "H(X)+H(Y)-H(X,Y):  0.217362115629\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The right/left bit\n",
      "Now we begin to look at our comedy_comparisons.train dataset.  (Note: We will only use the training data until the very end of the project, when we will test our model on the test set.)  But as a warm-up exercise, let's first look at an odd feature of the dataset.  Each record is a triple representing one vote, and it consists of two video IDS, followed by either \"right\" or \"left\" to indicate the winner of the vote.  It might appear that we could normalize the data by interchanging the two ID's whenever the third entry was \"left\" and dropping the third entry.  Then the winner would always appear in column 2.  But we lose a bit of data by doing this, namely: was the winner originally in column 1 (\"left\") or column 2 (\"right\")?  So the first question is, do we actually lose any information by\n",
      "normalizing?  \n",
      "\n",
      "An obvious thing to do is to compute the number of right winners minus the number of left winners:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv('Data/comedy_comparisons/comedy_comparisons.train', names=['left', 'right', 'winner'])\n",
      "delta_wins = 0\n",
      "for winner in data['winner']:\n",
      "    if winner == 'right':\n",
      "        delta_wins += 1\n",
      "    else:\n",
      "        delta_wins -= 1\n",
      "print \"right_wins - left_wins = \",delta_wins,\" out of \",len(data),\" votes\"\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "right_wins - left_wins =  32241  out of  912969  votes\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, 32241 looks like a fairly big number, but remember that there were almost a million votes.  So maybe this is just a random fluctuation.  We can test that hypothesis by noting that delta_wins is the sum of $N$ (presumably) i.i.d. flat random binary variables equal to $\\pm 1$. By the Central Limit Theorem, the sum is approximately normally distributed with mean zero and variance $N$.  So converting to sigmas, we get $ 32241/\\sqrt{912969} = 33.7\\sigma$.  Since the normal tail probability of a $33.7\\sigma$ result is zero to many decimal places, we conclude that there is a small but significant right-side bias.  Was this somehow the result of putting funnier videos on the right side more often?  Not according to the [data set information](http://archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data), which says that the right/left positions were \"randomly chosen\".  Good news!  We now have a predictor, namely \"the right-hand video wins\",  with what we will call a  *bias* of $32241/912969 = .035~$, *i.e.* it is correct  $(1 + .035)/2 = 51.75\\%~$ of the time on the training set.  Hopefully, this will remain true on the test set as well (but NO PEEKING!!) \n",
      "\n",
      "How much information does the right/left bit, call it $Y$, have about the won/lost bit, call it $X$? Well, $X$ is completely flat, because the number of wins always equals the number of losses. So there is one bit of entropy.  But conditioning on $Y = right$, the distribution becomes $(.5175,.4825)$ which has .999 bits of entropy.  For $H(X\\mid Y = left)$ we get the same thing, and since we're taking the flat average, the mutual information is .001 bits, which is not too exciting.  But perhaps we should look more carefully, because there might be a few funny videos which, for some reason, look much funnier when they're on the right side. To do this, we have to make separate counts for each video.  \n",
      "\n",
      "##Contingency Tables\n",
      "Given two random variables $X$ and $Y$, we can estimate their joint distribution from data by forming a matrix of counts $ C = [c_{xy}]$ where $c_{xy}~$ is the number of times that $X = x$ and $Y = y$. So the first column is the count of wins and losses\n",
      "when the video appeared on the left, and the second column is the won/lost counts when it appeared on the right.  Such a matrix is called a *contingency table*.  Dividing each entry by the sum of all the counts we get a sample joint distribution.  But before going any further, we need to discuss an important point about converting counts to probabilities.  \n",
      "\n",
      "###Flattening the counts\n",
      "What happens if one of the counts is zero?  If you simply divide by the total count as above, thereby setting the probability to zero, you are saying that since a particular outcome has not occurred in the past, it will *never* occur in the future.  BIG MISTAKE!  You have violated Goldschmidt's first rule of modeling:  **Sample probabilities are never, ever, zero!** So, what do we do with a zero count? A fair amount of research has been done on the question of how to properly convert counts to probabilities.  If you're a Bayesian, this is just a question of finding an appropriate prior probability distribution on the simplex (the Dirichlet distribution is a popular choice).  But for the frequentists in the crowd, here is a very simple, but surprisingly effective solution:  add one to each count.  Since we're not about to get bogged down in Bayesian complexity here, we will just add one.  This procedure is conservative in that small probabilities are increased a little and big probabilities are reduced a little.  As the size of the counts grow, the flattening effect asymptotically disappears.  A simple implementation is to just initialize the counts to one instead of zero before looping through the data.  In general, flattening is nothing more than setting aside a small amount of probability to account for as yet unseen future events.\n",
      "\n",
      "Moving on, the next step is to build a dictionary of flattened contingency tables, indexed by unique video ID.  (Note: I have pushed a file called \"unique_ids\" containing all the unique video IDs to the repository.)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cont_tbl = dict() # we will get a contingency table for each unique video id\n",
      "file = open('unique_ids',\"r\")\n",
      "ids = [x.rstrip() for x in file.readlines()]\n",
      "\n",
      "# ok, we have a list of the unique ids\n",
      "for id in ids:\n",
      "    cont_tbl[id] = [[1.0,1.0],[1.0,1.0]] # flatten the table\n",
      "j = 0\n",
      "for i in data.index:\n",
      "    # row 0: won, row 1: lost\n",
      "    # col 0: left, col 1: right\n",
      "    if data['winner'][i] == 'left':\n",
      "        cont_tbl[data['left'][i]][0][0] += 1\n",
      "        cont_tbl[data['right'][i]][1][1] += 1\n",
      "    else:\n",
      "        cont_tbl[data['left'][i]][1][0] += 1\n",
      "        cont_tbl[data['right'][i]][0][1] += 1\n",
      "    j += 1\n",
      "print \"We have \",len(ids),\" contingency tables\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We have  18474  contingency tables\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now if we divide each table by its sum, we can compute $I(X,Y) = H(X) - H(X|Y)$ for each video.  This tells us the information gain about $X$ (won/lost) that we get by specifying $Y$ (right/left).  Sometimes the *relative information gain* $I(X,Y)/H(X)$\n",
      "is more meaningful.  This is the fraction of the total information about $X$ that we get from $Y$.\n",
      "\n",
      "So why didn't we immediately convert to probabilities by dividing each table by its sum?  After all, we only need the joint distribution to compute mutual information, right?  True, but there's more to the story.  We need to evaluate the *significance* of each table.  Tables with a small number of counts will yield unreliable distributions due to random effects.  So how many votes do we need before we can place very much stock in the sample mutual information?  We're interested in the two columns of our table, because they are the conditional distributions on won/lost given right or left.  Just as in our first computation of bias, the bias for each column can be modeled as $b = S/N$, where S is the sum of $N$ i.i.d. random variables $X_i = \\pm 1$  each of mean $b$, so that $Pr(X_i = 1)$ is $p = (1+b)/2$.  Then the std. deviation of S is $\\sqrt{(1-b^2)N}$ and we get \"error bars\" for $b$ of size roughly $\\sqrt{(1-b^2)/N}$.  So it looks like one decimal place accuracy corresponds to something like 100 votes (in each column).  To keep things simple, let's make a list of the \"popular\" videos that got at least 200 votes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "popular = [[id,cont_tbl[id]] for id in ids if sum(cont_tbl[id]) >= 200]\n",
      "print \"there are \",len(popular),\" popular videos and they got a total of \",\\\n",
      ".5*sum([popular[i][1] for i in range(0,len(popular))]),\" out of \",len(data), \" votes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "there are  1325  popular videos and they got a total of  714687.0  out of  912969  votes\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow! 7% of the videos got 78% of the votes! Now we can compute the mutual information for each of the \"popular\" videos.  But before we do, this is a good time to introduce another very classical independence test.\n",
      "## The Contingency Chi-Square Test\n",
      "Given a set of observed counts $O_1,O_2,\\dots,O_n$ and expected values for these counts $E_1,E_2,\\dots,E_n$ we compute \n",
      "$$\n",
      "\\chi^2 := \\sum_{i=1}^n\\frac{(O_i - E_i)^2}{E_i}.\n",
      "$$]]\n",
      "The idea is that, analagous to what we've already seen above, each term $(O_i-E_i)/\\sqrt{E_i}$ should be approximately normally distributed with mean 0 and variance 1.  So $\\chi^2$ is a sum of squares of (approximately) normal random variables.  But there may be some constraints on these variables.  For example, if the sum of all the counts is always 100, say, then there are really only $n-1$ \"degrees of freedom\".  Without dwelling on this sometimes tricky point, the \"degrees of freedom\" in a chi-square statistic (often denoted $\\nu$) is important.  The reason is that the tail probability function $t(x,\\nu) = Pr(\\chi^2 \\ge x)$,\n",
      "often called the *p-value*, can be computed.  So we can quantify how surprising (unexpected) a given set of counts is. For example, $1/t(x,\\nu)$ is often interpreted as the number of experiments we would expect to run if the $E_i$ were \"correct\" (i.e. if the so-called null hypothesis were true)  before seeing a $\\chi^2$ statistic as big or bigger than the one we got.\n",
      "\n",
      "For a contingency table we want to know whether the two sample marginal distributions $p$ and $q$ are independent, so the expected values are those that would arise if they were, namely $E_{ij} = p_iq_jN$. The number of degrees of freedom is a bit tricky, but in a $2\\times{2}$ table with given row and column sums, there is only one degree of freedom, because knowing any one count determines all the others.  So let's run the test on the popular videos.  In fact, let's append a column of chi-square p-values to the popular table, and save all the tables for which the chi-square p-value is less than 1/18,474.  Why this threshold?  Because you would expect to see less than 1 such table in a list of 18,474 tables generated under the null hypothesis that the marginals are independent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats\n",
      "def contingency_chisq(a):\n",
      "    #print a\n",
      "    #return [0,0]\n",
      "    p = a[0][0] + a[0][1]\n",
      "    q = a[0][0] + a[1][0]\n",
      "    n = sum(a)\n",
      "    obs = np.array([a[0][0],a[0][1],a[1][0],a[1][1]])\n",
      "    exp = np.array([p*q/n,p*(n-q)/n,(n-p)*q/n,(n-p)*(n-q)/n])\n",
      "#    print \"observed: \", obs\n",
      "#    print \"expected: \", exp\n",
      "    return scipy.stats.chisquare(obs,exp,1)\n",
      "#print popular[0:2]\n",
      "#print \"x[1]: \",[x[1] for x in popular[0:2]]\n",
      "popular1 = sorted([[contingency_chisq(x[1])[1],x[0],x[1]] for x in popular], key=my_key)\n",
      "hits = [x for x in popular1 if x[0] < 1.0/18474]\n",
      "print \"there were \",len(hits),\" significant chi-square hits: \", hits\n",
      "\n",
      "#print \"top ten chisq pvals: \", chisq[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "there were  8  significant chi-square hits:  [[4.8424219469490673e-12, 'W9y6nwBwwyQ', [[13329.0, 14381.0], [11935.0, 11325.0]]], [5.6996102221957085e-07, 'X1hz0RKKzpQ', [[4239.0, 4717.0], [6034.0, 5777.0]]], [1.794783663292132e-06, 'FMhrCoyeRVI', [[7034.0, 7580.0], [8585.0, 8235.0]]], [4.1373551375157852e-06, 'bOcugYjrvLg', [[9721.0, 10093.0], [8976.0, 8403.0]]], [4.4853604887888478e-06, 'ar0G-EcJDnw', [[5363.0, 5642.0], [5377.0, 4937.0]]], [2.225953113670297e-05, 'LLaKkC5U9Po', [[12871.0, 13404.0], [11759.0, 11264.0]]], [2.6185409388006979e-05, '1JZoXX5BrNA', [[108.0, 146.0], [162.0, 96.0]]], [3.2437137387597733e-05, 'HRaEcDT3uqw', [[7948.0, 8408.0], [7976.0, 7621.0]]]]\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's compute the mutual information for the significant chi-square hits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mutual_info(d): # compute mutual info for 2x2 distr. d\n",
      "    p = d[0,0] + d[0,1]\n",
      "    q = d[0,0] + d[1,0]\n",
      "    \n",
      "    try:\n",
      "        mut_info = d[0,0]*log(d[0,0]/(p*q))\\\n",
      "            + d[0,1]*log(d[0,1]/(p*(1-q)))\\\n",
      "            + d[1,0]*log(d[1,0]/((1-p)*q))\\\n",
      "            + d[1,1]*log(d[1,1]/((1-p)*(1-q)))\n",
      "    except:\n",
      "        print d\n",
      "        return [1,0,0]\n",
      "    info = np.zeros(3)\n",
      "    info[0] = mut_info/log(2) #mutual info in bits\n",
      "    info[1] = -mut_info/(p*log(p)+(1-p)*log(1-p))# relative information gains\n",
      "    info[2] = -mut_info/(q*log(q)+(1-q)*log(1-q))\n",
      "    return info\n",
      "\n",
      "minfo = [mutual_info(x[2]/sum(x[2]))[1] for x in hits] # relative info gain in X given Y\n",
      "print \"relative info gain for chi-square hits: \", minfo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "relative info gain for chi-square hits:  [0.00074163044442668866, 0.0010130032114768063, 0.00060949725997176663, 0.00048236521464107462, 0.00083416008115283827, 0.00031451661011852318, 0.02993623874223315, 0.00046692452976976757]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So it doesn't look like knowing $Y$ really helps with $X$ very much. But we have clearly illustrated the difference between $\\chi^2$ and mutual information.  The former only tells you that there's a very small chance that the marginals are really independent.  But it says nothing about how dependent they are.  The latter tells you how dependent the *sample* marginals are for a particular table, but says nothing about the chance of that happening at random if the \"real\" marginals were in fact independent. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Probability Models\n",
      "We're going to look at pairs of videos and the metadata associated with them and try to predict which one will be funnier.  So you might think that we want an algorithm that inputs the metadata and outputs 0 if it thinks the left-hand video is funnier or 1 if it thinks the right-hand video is funnier.  But since this is a statistics class, we will instead estimate the *probability*, $p$, that the right-hand one is funnier.  Besides, this turns out to work better in most cases.  One obvious reason for this is that there's clearly a big difference between the outputs $p = .51$ and $p = .95$, even though both of these say that the right-hand video is funnier.  But there are many other not-so-obvious reasons why probability models are better than deterministic models for this sort of problem, and hopefully we'll see some of them as we go along. The model we are going to use is called *logistic regression*.  The python package statsmodels.api has a good implementation, and there is an excellent example of it [here](http://blog.yhathq.com/posts/logistic-regression-and-python.html).  This is a highly instructive ipython notebook which not only shows logistic regression in action but also demonstrates some cool features of pandas.  Please read it.\n",
      "Now, if this were an undergraduate course, we could just use the python package without worrying about how it works.\n",
      "But this is a graduate course, and you are supposed to be Masters of Statistics!  So we're going to spend some time learning what's really going on.\n",
      "\n",
      "So, what exactly is a *probability model*?  Basically, it's just a real-valued non-negative function $L(y,x;\\theta)$,\n",
      "where $y$ is the quantity we're trying to predict,  $x$ is a vector of $n$ real numbers which we will call *predictors*, and $\\theta$ is a vector of model parameters.  The value of $L$ is the conditional likelihood of $y$ given $x$ and $\\theta$. \n",
      "There is no standard terminology -- in machine learning $x$ is often called the \"context\" and $y$ the \"goal\". Typically, we are given a data set $(y_i,x_i),~i = 1,2,\\dots,N$ called the \"training set\" which we use to try to figure out the best set of parameters $\\theta$. This is called *training the model*.\n",
      "\n",
      "Assuming that the pairs $(y_i,x_i)$ are samples of independent random variables, the likelihood of the entire training set is the product\n",
      "$$\n",
      "L^* = \\prod_{i=1}^N L(y_i,x_i;\\theta),\n",
      "$$\n",
      "and the \"best\" value of theta is usually taken to mean the value that maximizes $L^*$. So the training process is called *Maximum Likelihood Estimation* or MLE for short. It's usually easier to maximize \n",
      "$$\n",
      "\\log(L^*) = \\sum_{i=1}^N\\log(L(y_i,x_i;\\theta)\n",
      "$$\n",
      "and the resulting value of $\\theta$ is the same in either case (why?).\n",
      "\n",
      "Once we have trained the model and we have values for $\\theta$, we can use it to compute the conditional probability distribution of $y$ given a \"test\" predictor vector $x_j$. This distribution is our prediction for $y$ given $X_j$.  Finally, we can test our prediction by computing the probability of the test value $y_j$.\n",
      "\n",
      "Logistic Regression is a particular example of MLE, but before we get to this, it's instructive to begin with the canonical example.\n",
      "\n",
      "##Linear Regression\n",
      "The simplest possible relationship between $y$ and $x$ would be linear: \n",
      "\n",
      "$$\n",
      "y_i = x_i\\cdot\\theta, \\quad 1\\le i\\le N,\n",
      "$$\n",
      "\n",
      "for some magic vector $\\theta$.\n",
      "But in real life, such an $\\theta$ does not exist.  But we might pretend that there is some additive measurement noise $\\epsilon_i$ which is responsible for this.  So perhaps a magic $\\theta$ really does exist, but it is hidden in noise:\n",
      "\n",
      "$$\n",
      "y_i = \\theta\\cdot{X}_i + \\epsilon_i,\\quad 1\\le i\\le N.\n",
      "$$\n",
      "\n",
      "And if we further pretend that the $\\epsilon_i$ are independent and normally distributed  with mean zero and known variance $\\sigma_i^2$, we get a probability model:\n",
      "\n",
      "$$\n",
      "L(y_i,x_i;\\theta) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}}e^{-(\\epsilon_i/2\\sigma_i)^2} = \\frac{1}{\\sigma_i\\sqrt{2\\pi}}\\exp[-\\frac{1}{2\\sigma_i^2}(y_i-x_i\\cdot\\theta)^2]\n",
      "$$\n",
      "\n",
      "with log-likelihood function\n",
      "\n",
      "$$\n",
      "\\log(L^*) = \\sum_{i = 1}^N -\\log(\\sigma_i\\sqrt{2\\pi}) - \\frac{1}{2}\\sum_{i=1}^N\\frac{(y_i-x_i\\cdot\\theta)^2}{\\sigma_i^2}.\n",
      "$$\n",
      "\n",
      "Since the first sum is independent of $\\theta$, the maximum likelihood problem here is just the classical weighted least-squares problem: choose $\\theta$ to minimize (because of the minus sign) the right-hand sum of squares.  Hopefully you've seen this before. We get linear equations in the $n$ parameters $\\theta = \\theta_1,\\dots,\\theta_n$ by setting the partial derivatives of the log-likelihood to zero:\n",
      "\n",
      "$$\n",
      "\\frac{\\partial{\\log(L^*)}}{\\partial{\\theta_j}} = 0 \\quad 1\\le j \\le n.\n",
      "$$\n",
      "\n",
      "These equations can be written in matrix notation as:\n",
      "\n",
      "$$\n",
      "X^tX\\theta = X^ty,\n",
      "$$\n",
      "\n",
      "where $X$ is the $N\\times{n}$ matrix whose $i^{th}$ row is $x_i/\\sigma_i$ and $y$ is the $N\\times{1}$ vector whose $i^{th}$ component is $y_i/\\sigma_i$ as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The solution is\n",
      "\n",
      "$$\n",
      "\\theta = (X^tX)^{-1}X^ty.\n",
      "$$\n",
      "\n",
      "Now this is all very well, but how often are we going to find a dataset $y,X$ where $y$ differs from a linear function of \n",
      "$X$ by a normally distributed *residual* $\\epsilon$?  The answer is not very often, but even when there's some reason to\n",
      "believe that the noise is \"white\", it is usually not independent across samples, but instead comes in bursts. That's why I said we had to pretend when we wrote down the model.  This is an instance of Goldschmidt's second rule of modeling: **All models lie!**  We're always making assumptions that we know are not correct.  We just hope that they don't matter too much.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}