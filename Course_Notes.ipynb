{
 "metadata": {
  "name": "Course_Notes"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Information Theory 101\n",
      "Information Theory was invented by Claude Shannon in a remarkable 1948 paper entitled \"A Mathematical Theory of Communication\".\n",
      "Today its uses extend far beyond communication.  We are going to use it in several ways in this course, so we begin with an overview of the basic elements of the theory.\n",
      "\n",
      "##Entropy\n",
      "Suppose that $X$ is a discrete random variable taking values $x_i$ with probability $p_i$ for $1\\le i\\le n$.\n",
      "We want to measure the uncertainty in $X$, but it's not clear how this should be defined.  Not so obviously, we put\n",
      "$$\n",
      "H(X) = -\\sum_{i=1}^np_i\\log(p_i).\n",
      "$$\n",
      "where $0\\log(0) = \\lim_{p->0}p\\log(p) = 0$.  In other words, $H(X)$ is the expected value of $-\\log(p_i)$.  We call $H(X)$ the *entropy* of $X$.  The base of the logarithm determines the units of $H(X)$.  For base 2, the most common choice, the units are bits.  For base $e$, the units are \"nats\", and for base 10, the units are \"bans\".\n",
      "\n",
      "It's clear that entropy is always non-negative, and that it is zero precisely when $p_i = 1$ for some $i$ and $p_j = 0$ for all $j \\neq i$.  Of course, this is exactly the case where there is no uncertainty at all because we know in advance of any experiment that $X = x_i$. Furthermore, it's a fairly easy calculus exercise to see that the maximum possible value of $H(X)$ is $\\log(n)$, and that it occurs precisely when $p_i = 1/n$ for all $i$. This is clearly the case of maximum uncertainty, so again our definition behaves properly.\n",
      "\n",
      "Let's see how this works in practice.  We'll take $n = 16$ and generate a random discrete distribution as an ndarray $p$ of length 16 which will define a random variable $X$ taking values $\\{1,2,\\dots,16\\}$, and we'll then compute its entropy:    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "from math import log\n",
      "\n",
      "random.seed(1234)\n",
      "m = 16\n",
      "\n",
      "p = [random.random() for i in range(0,m)]\n",
      "s = sum(p)\n",
      "p = [p[i]/s for i in range(0,m)]\n",
      "\n",
      "HX = -sum(q*log(q) for q in p)\n",
      "print \"p: \",p\n",
      "print \"Entropy of p: \",HX/log(2),\" bits per symbol\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "p:  [0.11844852881133701, 0.054016179819892235, 0.0009181544423016086, 0.11164919838315228, 0.1151167922585752, 0.07135780139997963, 0.08230680880969994, 0.010287467646092811, 0.09393989001872718, 0.029023402019856318, 0.0037765659759976117, 0.09667197073912931, 0.04241665770663092, 0.07638936692479224, 0.07547436106353045, 0.018206853980305106]\n",
        "Entropy of p:  3.63807761268  bits per symbol\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Huffman Trees\n",
      "So far, so good, but there are infintely many different possible functions $H(X)$ with the above two properties.\n",
      "Here is some intuition about why our definition is what it is.\n",
      "\n",
      "Suppose you have sampled $X$ and you have a value $x_i$.  I want to find out which $x$ you have by means of a sequence of yes-no questions. Each question can depend on the answers to the previous questions, just like the game \"twenty questions\".  So the sequence of questions forms a path through a binary tree which ends at a leaf when all values but one have been eliminated.  \n",
      "\n",
      "It's pretty clear that the optimal way to design the tree is to choose a subset $S$ of values whose total probability is $1/2$, or as close to $1/2$ as possible, and ask the question \"Is $x$ in $S$?\"\n",
      "The answer will eliminate either $S$ or its complement, and then we can renormalize the remaining probabilities to\n",
      "sum to one and recurse.  Intuitively, the answer to each question reduces the uncertainty in $x$ by one bit.  We usually say that each question provides one bit of information about $x$.  This is actually the definition of information:  An experiment yields $b$ bits of information about $X$ when it's outcome reduces the uncertainty in $X$ by $b$ bits.\n",
      "\n",
      "Now we can see where our formula for $H(X)$ comes from.  For the sake of simplicity, lets assume that at each step, it's possible to choose $S$ with total probability exactly $1/2$.  Since $x_i$ is never eliminated (because you aren't allowed to lie) its probability is doubled at each recursive step because we divide all remaining probabilities by $1/2$ at each step. So we reach the leaf $x = x_i$ precisely when the renormalized probability of $x_i$ reaches 1.  Solving the equation $2^kp_i = 1$, we see that it takes (approximately) $-\\log_2(p_i)$ questions to determine that $x = x_i$.  So $H(X)$ is the expected number of (optimal) questions needed to determine one random value of $X$. Of course, if we have a sequence of $N$ random values of $X$, we expect to ask $H(X)N$ questions to learn all\n",
      "the values.  So entropy is often thought of as a rate, in bits (or questions) per value of $X$. \n",
      "\n",
      "##Data Compression\n",
      "There's another use for the above tree which explains what the whole subject has to do with communications. Suppose that the values $x_i$ of $X$ are symbols which we concatenate to form a text of some length, say $N$.  For example, with $n = 16$ they might be the 16 different nibbles. Further, suppose that $p_i$ is the probability of occurence of $x_i$ in our text. If we replace the branch labels \"yes\" and \"no\" with $1$ and $0$ respectively, the path from the root to $x_i$ defines a binary codeword $c_i$ of length $-\\log_2{p_i}$ bits (in the ideal case).  So we can replace each occurence of $x_i$ in our text with $c_i$, and you can see (exercise!) that instead of the original $4N$ bits needed for our text of $N$ nibbles, we have compressed the text to roughly $H(X)N$ bits.  Furthermore, it is clear that this code is *prefix-free*, which means that no codeword is a prefix of any other codeword.  This means that we can decode a string of concatenated codewords without needing any special stop symbol to signal the end of the current codeword. Shannon was originally interested in data compression (which he called \"source coding\") to improve the efficiency of digital communications.\n",
      "\n",
      "##Huffman's Algorithm\n",
      "How do we actually build the tree?  At first glance, it looks like in order to take the first step, we have to inspect all possible subsets $S$ in order to find one whose total probability is as close to $1/2$ as possible.  But this procedure has exponential work and is completely impractical for any reasonably large value of $n$, like $n=256$ for example.  Fortunately, a graduate student at MIT by the name of David Huffman discovered an efficient algorithm which has complexity $O(n\\log{n})$.  The trick is to build the tree recursively from the bottom up, rather than from the top down.  First, sort the $p_i$ so that $p_1\\le p_2\\le \\dots \\le p_n$.  Let $y_1$ be the parent node of $x_1$. Now a moment's thought will convince you that we must have $y_1 = ${$x_1,x_2$}  because this makes $p_1$ as close to \n",
      "$Pr(y_1)/2$ as possible.  Now let $X^{(1)}$ be a new random variable taking values $y_1,x_3,\\dots,x_n$ with probabilities $p_1+p_2,p_3,\\dots,p_n$.  Re-sort the probabilities (note: work is $O(\\log{n})$ to insert $p_1+p_2$ into the sorted list\n",
      "$p_3,\\dots,p_n$) and recurse.  It's pretty clear that this algorithm produces optimal trees, which is why they are called Huffman trees.\n",
      "\n",
      "Let's see how this works by writing a program to find an optimal tree for the random variable $X$ that we generated above, and compute the average distance of each leaf from the root: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import copy\n",
      "from math import log\n",
      "class Tree:\n",
      "    def __init__(self,left,right,data):\n",
      "        self.left = left # left and right are subtrees for nodes and None for leaves\n",
      "        self.right = right\n",
      "        self.data = data # data is a set of symbols\n",
      "\n",
      "def my_key(x): # needed to sort pairs by first element\n",
      "    return x[0]\n",
      "class Huffman:\n",
      "    node = [] # where the nodes are stored\n",
      "    root = Tree(None,None,None)\n",
      "    def __init__(self,p):\n",
      "        n = len(p)\n",
      "        leaf = [[p[i], Tree(None,None,set([i]))] for i in range(0,n)] # initialize the leaves\n",
      "        node = copy.deepcopy(leaf) # initialize node list with the leaves\n",
      "        leaf.sort(key=my_key) # sort the leaves by their probability\n",
      "        new = []\n",
      "        \n",
      "        for i in range(0,n-1): # link the two remaining least likely \"leaves\" into a new node\n",
      "            left = leaf[i]\n",
      "            right = leaf[i+1]\n",
      "            new = [left[0]+right[0],Tree(left[1],right[1],left[1].data | right[1].data)]\n",
      "            node.append(new) # copy it into the node list\n",
      "            j = i+1\n",
      "            while j < n-1 and new[0] > leaf[j+1][0]: \n",
      "                leaf[j] = copy.deepcopy(leaf[j+1]) # move existing less likely \"leaves\" downward\n",
      "                j += 1\n",
      "            leaf[j] = new #insert the new node as a recursive \"leaf\"\n",
      "        self.node = node # save the node list and the root of the tree\n",
      "        self.root = new[1]\n",
      "        \n",
      "    def encode(self,i): # encode symbol i\n",
      "        node = self.root\n",
      "        code = \"\"\n",
      "        while node.left != None:\n",
      "            if i in node.left.data:\n",
      "                node = node.left\n",
      "                code += '0'\n",
      "            else:\n",
      "                node = node.right\n",
      "                code += \"1\"\n",
      "        return code\n",
      "    \n",
      "    def decode(self,s): # decode word s\n",
      "        node = self.root\n",
      "        for i in range(0,len(s)):\n",
      "            if s[i] == '0':\n",
      "                node = node.left\n",
      "            else:\n",
      "                node = node.right\n",
      "        return node.data\n",
      "                   \n",
      "Huff = Huffman(p)\n",
      "print \"Code Table:\\n\"\n",
      "H = 0\n",
      "for i in range(0,len(p)):\n",
      "    s = Huff.encode(i)\n",
      "    ii = Huff.decode(s)\n",
      "    print i,\" code: \",s,\" decode: \",ii,\" p: \",p[i],\" -log(p): \",-log(p[i])/log(2)\n",
      "    H += len(s)*p[i]\n",
      "print \"\\nCoding entropy: \",H,\" bits per symbol\"\n",
      "  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Code Table:\n",
        "\n",
        "0  code:  100  decode:  set([0])  p:  0.118448528811  -log(p):  3.07766781518\n",
        "1  code:  0001  decode:  set([1])  p:  0.0540161798199  -log(p):  4.21046457788\n",
        "2  code:  10101000  decode:  set([2])  p:  0.000918154442302  -log(p):  10.0889755305\n",
        "3  code:  010  decode:  set([3])  p:  0.111649198383  -log(p):  3.16295520192\n",
        "4  code:  011  decode:  set([4])  p:  0.115116792259  -log(p):  3.11882979798\n",
        "5  code:  1011  decode:  set([5])  p:  0.0713578014  -log(p):  3.8087850246\n",
        "6  code:  1110  decode:  set([6])  p:  0.0823068088097  -log(p):  3.60284440761\n",
        "7  code:  1010101  decode:  set([7])  p:  0.0102874676461  -log(p):  6.60296829637\n",
        "8  code:  1111  decode:  set([8])  p:  0.0939398900187  -log(p):  3.41211828523\n",
        "9  code:  10100  decode:  set([9])  p:  0.0290234020199  -log(p):  5.10663955288\n",
        "10  code:  10101001  decode:  set([10])  p:  0.003776565976  -log(p):  8.04870929391\n",
        "11  code:  001  decode:  set([11])  p:  0.0966719707391  -log(p):  3.37075853727\n",
        "12  code:  0000  decode:  set([12])  p:  0.0424166577066  -log(p):  4.55922524406\n",
        "13  code:  1101  decode:  set([13])  p:  0.0763893669248  -log(p):  3.71048435456\n",
        "14  code:  1100  decode:  set([14])  p:  0.0754743610635  -log(p):  3.72786955128\n",
        "15  code:  101011  decode:  set([15])  p:  0.0182068539803  -log(p):  5.77937453375\n",
        "\n",
        "Coding entropy:  3.6731919044  bits per symbol\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Conditional Entropy and Mutual Information\n",
      "In addition to data compression and many other uses in communication theory, information theory turns out to be very useful for studying dependencies between pairs of random variables, and this will be our actual application.\n",
      "\n",
      "So suppose we have another random variable $Y$ taking values $y_j$ with probability $q_j$ for $1\\le j\\le m$.\n",
      "Then there is a joint distribution $r_{ij} := Pr(X = x_i \\& Y = y_j)$ with marginals\n",
      "\n",
      "$$ \n",
      "p_i = \\sum_jr_{ij}\\quad(1\\le i\\le n) \n",
      "$$\n",
      "and\n",
      "$$\n",
      "q_j = \\sum_ir_{ij}\\quad(1\\le j\\le m).\n",
      "$$\n",
      "The *joint entropy* is just the entropy of the joint distribution:\n",
      "\n",
      "$$\n",
      "H(X,Y) := -\\sum_{i,j}r_{ij}\\log(r_{ij}).\n",
      "$$\n",
      "Given a value $Y = y_j$, the *conditional entropy* is just the entropy of the conditional distribution:\n",
      "\n",
      "$$\n",
      "H(X\\mid y_j) = -\\sum_i\\frac{r_{ij}}{q_j}\\log(\\frac{r_{ij}}{q_j}),\n",
      "$$ \n",
      "and we define\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) := \\sum_jq_jH(X\\mid y_j) = -\\sum_{ij}r_{ij}(\\log{r_{ij}} - \\log{q_j})\n",
      "= H(X,Y) + \\sum_j q_j\\log{q_j}\n",
      "= H(X,Y) - H(Y).\n",
      "$$\n",
      "\n",
      "This is a very nice result, for two reasons.  First, since entropy is always non-negative, we see that $H(X,Y) \\ge H(Y)$.\n",
      "More importantly, by symmetry we get\n",
      "\n",
      "$$\n",
      "H(X\\mid Y) + H(Y) = H(X,Y) = H(Y\\mid X) + H(X)\n",
      "$$\n",
      "\n",
      "so that we can define\n",
      "\n",
      "$$\n",
      "I(X,Y) := H(X) - H(X\\mid Y) = H(Y) - H(Y\\mid X) = H(X) + H(Y) - H(X,Y).\n",
      "$$\n",
      "\n",
      "So $I(X,Y)$ is the expected reduction in the entropy of $X$ given a random sample from $Y$, or as it's often loosely described, the information about $X$ obtained by knowing $Y$. By symmetry, it is also the information about $Y$ obtained by knowing $X$, so we call $I(X,Y)$ the *mutual information* of $X$ and $Y$.  A simple calculation (exercise!) shows that\n",
      "$$\n",
      "I(X,Y) = \\sum_{ij}r_{ij}\\log(\\frac{r_{ij}}{p_iq_j}).\n",
      "$$\n",
      "\n",
      "It can be shown that $I(X,Y) \\ge 0$.  (This is intuitively clear, because knowing a value of $Y$ can't possibly *increase* our uncertainty about $X$.  Nevertheless, the proof isn't completely trivial.)  It's not hard to see (exercise!) that equality holds iff $r_{ij} = p_iq_j$ for all $i,j$. In other words, the mutual information of two random variables is zero precisely when they are independent.  More generally, $I(X,Y)$ is a measure of the dependence between $X$ and $Y$. How big can it get?  From the above equations it follows easily that $I(X,Y)$ is maximized for a fixed $X$ precisely when $Y = X$ (exercise!).\n",
      "\n",
      "To illustrate these ideas, we will generate a 16x10 ndarray $r$ of non-negative reals such that the $i^{th}$ row sum is $p_i$ for $1\\le i\\le 16$.  Then we'll call the $j^{th}$ column sum $q_j$ (and show that the $q_j$ sum to 1), giving us another random variable $Y$ taking values $\\{1,2,\\dots,10\\}$.  Then we'll compute $H(Y), H(Y\\mid X), H(X\\mid Y), H(X,Y)$, and $I(X,Y)$, and verify the above formulas: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = 16\n",
      "n = 10\n",
      "r = [[random.random() for j in range(0,n)] for i in range(0,m)]\n",
      "for i in range(0,m):\n",
      "    f = p[i]/sum(r[i])\n",
      "    r[i] = [r[i][j]*f for j in range(0,n)] \n",
      "q = [sum(r[i][j] for i in range(0,m)) for j in range(0,n)]\n",
      "\n",
      "print \"sum(q): \",sum(q)\n",
      "HY = -sum(q[j]*log(q[j]) for j in range(0,10))\n",
      "HYx = [-sum(r[i][j]/p[i]*log(r[i][j]/p[i]) for j in range(0,n)) for i in range(0,m)]\n",
      "HYbarX = sum(HYx[i]*p[i] for i in range(0,m))\n",
      "HXy = [-sum(r[i][j]/q[j]*log(r[i][j]/q[j]) for i in range(0,m)) for j in range(0,n)]\n",
      "HXbarY = sum(HXy[j]*q[j] for j in range(0,n))\n",
      "HXY = -sum(sum(r[i][j]*log(r[i][j]) for i in range(0,m)) for j in range(0,n))\n",
      "IXY = sum(sum(r[i][j]*log(r[i][j]/(p[i]*q[j])) for i in range(0,m)) for j in range(0,n))\n",
      "print \"H(X): \",HX\n",
      "print \"H(Y): \",HY\n",
      "print \"H(X|y_j): \",HXy\n",
      "print \"H(Y|x_i): \",HYx\n",
      "print \"H(X|Y): \", HXbarY\n",
      "print \"H(Y|X): \", HYbarX\n",
      "print \"H(X,Y): \", HXY\n",
      "print \"I(X,Y): \", IXY\n",
      "print \"H(X)-H(X|Y): \", HX-HXbarY\n",
      "print \"H(Y)-H(Y|X): \", HY-HYbarX\n",
      "print \"H(X)+H(Y)-H(X,Y): \",HX+HY-HXY\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sum(q):  1.0\n",
        "H(X):  2.52172323989\n",
        "H(Y):  2.29368292785\n",
        "H(X|y_j):  [2.4427510219149453, 2.235845036448071, 2.3460216951593025, 2.2163516578034788, 2.116382767338068, 2.3817213388969853, 2.3871990939060694, 2.4619557570580244, 2.2202431242380527, 2.244426835573572]\n",
        "H(Y|x_i):  [1.9384284102687208, 2.2342510102645994, 1.941362754576079, 1.9960381055355085, 2.0659687257455186, 2.16406847157114, 2.2193644159677572, 2.0887624848154496, 1.985141053650841, 2.0746777877254106, 2.222238191035729, 2.008733846503033, 2.174604446461029, 2.180497339279629, 2.0842449781694787, 2.1746040786742187]\n",
        "H(X|Y):  2.30436112426\n",
        "H(Y|X):  2.07632081223\n",
        "H(X,Y):  4.59804405211\n",
        "I(X,Y):  0.217362115629\n",
        "H(X)-H(X|Y):  0.217362115629\n",
        "H(Y)-H(Y|X):  0.217362115629\n",
        "H(X)+H(Y)-H(X,Y):  0.217362115629\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The right/left bit\n",
      "Now we begin to look at our comedy_comparisons.train dataset.  (Note: We will only use the training data until the very end of the project, when we will test our model on the test set.)  But as a warm-up exercise, let's first look at an odd feature of the dataset.  Each record is a triple representing one vote, and it consists of two video IDS, followed by either \"right\" or \"left\" to indicate the winner of the vote.  It might appear that we could normalize the data by interchanging the two ID's whenever the third entry was \"left\" and dropping the third entry.  Then the winner would always appear in column 2.  But we lose a bit of data by doing this, namely: was the winner originally in column 1 (\"left\") or column 2 (\"right\")?  So the first question is, do we actually lose any information by\n",
      "normalizing?  \n",
      "\n",
      "An obvious thing to do is to compute the number of right winners minus the number of left winners:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv('Data/comedy_comparisons/comedy_comparisons.train', names=['left', 'right', 'winner'])\n",
      "delta_wins = 0\n",
      "for winner in data['winner']:\n",
      "    if winner == 'right':\n",
      "        delta_wins += 1\n",
      "    else:\n",
      "        delta_wins -= 1\n",
      "print \"right_wins - left_wins = \",delta_wins,\" out of \",len(data),\" votes\"\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "right_wins - left_wins =  32241  out of  912969  votes\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, 32241 looks like a fairly big number, but remember that there were almost a million votes.  So maybe this is just a random fluctuation.  We can test that hypothesis by noting that delta_wins is the sum of $N$ (presumably) i.i.d. flat random binary variables equal to $\\pm 1$. By the Central Limit Theorem, the sum is approximately normally distributed with mean zero and variance $N$.  So converting to sigmas, we get $ 32241/\\sqrt{912969} = 33.7\\sigma$.  Since the normal tail probability of a $33.7\\sigma$ result is zero to many decimal places, we conclude that there is a small but significant right-side bias.  Was this somehow the result of putting funnier videos on the right side more often?  Not according to the [data set information](http://archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data), which says that the right/left positions were \"randomly chosen\".  Good news!  We now have a predictor, namely \"the right-hand video wins\",  with what we will call a  *bias* of $32241/912969 = .035~$, *i.e.* it is correct  $(1 + .035)/2 = 51.75\\%~$ of the time on the training set.  Hopefully, this will remain true on the test set as well (but NO PEEKING!!) \n",
      "\n",
      "How much information does the right/left bit, call it $Y$, have about the won/lost bit, call it $X$? Well, $X$ is completely flat, because the number of wins always equals the number of losses. So there is one bit of entropy.  But conditioning on $Y = right$, the distribution becomes $(.5175,.4825)$ which has .999 bits of entropy.  For $H(X\\mid Y = left)$ we get the same thing, and since we're taking the flat average, the mutual information is .001 bits, which is not too exciting.  But perhaps we should look more carefully, because there might be a few funny videos which, for some reason, look much funnier when they're on the right side. To do this, we have to make separate counts for each video.  \n",
      "\n",
      "##Contingency Tables\n",
      "Given two random variables $X$ and $Y$, we can estimate their joint distribution from data by forming a matrix of counts $ C = [c_{xy}]$ where $c_{xy}~$ is the number of times that $X = x$ and $Y = y$. So the first column is the count of wins and losses\n",
      "when the video appeared on the left, and the second column is the won/lost counts when it appeared on the right.  Such a matrix is called a *contingency table*.  Dividing each entry by the sum of all the counts we get a sample joint distribution.  But before going any further, we need to discuss an important point about converting counts to probabilities.  \n",
      "\n",
      "###Flattening the counts\n",
      "What happens if one of the counts is zero?  If you simply divide by the total count as above, thereby setting the probability to zero, you are saying that since a particular outcome has not occurred in the past, it will *never* occur in the future.  BIG MISTAKE!  You have violated Goldschmidt's first rule of modeling:  **Sample probabilities are never, ever, zero!** So, what do we do with a zero count? A fair amount of research has been done on the question of how to properly convert counts to probabilities.  If you're a Bayesian, this is just a question of finding an appropriate prior probability distribution on the simplex (the Dirichlet distribution is a popular choice).  But for the frequentists in the crowd, here is a very simple, but surprisingly effective solution:  add one to each count.  Since we're not about to get bogged down in Bayesian complexity here, we will just add one.  This procedure is conservative in that small probabilities are increased a little and big probabilities are reduced a little.  As the size of the counts grow, the flattening effect asymptotically disappears.  A simple implementation is to just initialize the counts to one instead of zero before looping through the data.  In general, flattening is nothing more than setting aside a small amount of probability to account for as yet unseen future events.\n",
      "\n",
      "Moving on, the next step is to build a dictionary of flattened contingency tables, indexed by unique video ID.  (Note: I have pushed a file called \"unique_ids\" containing all the unique video IDs to the repository.)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cont_tbl = dict() # we will get a contingency table for each unique video id\n",
      "file = open('unique_ids',\"r\")\n",
      "ids = [x.rstrip() for x in file.readlines()]\n",
      "\n",
      "# ok, we have a list of the unique ids\n",
      "for id in ids:\n",
      "    cont_tbl[id] = [[1.0,1.0],[1.0,1.0]] # flatten the table\n",
      "j = 0\n",
      "for i in data.index:\n",
      "    # row 0: won, row 1: lost\n",
      "    # col 0: left, col 1: right\n",
      "    if data['winner'][i] == 'left':\n",
      "        cont_tbl[data['left'][i]][0][0] += 1\n",
      "        cont_tbl[data['right'][i]][1][1] += 1\n",
      "    else:\n",
      "        cont_tbl[data['left'][i]][1][0] += 1\n",
      "        cont_tbl[data['right'][i]][0][1] += 1\n",
      "    j += 1\n",
      "print \"We have \",len(ids),\" contingency tables\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We have  18474  contingency tables\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now if we divide each table by its sum, we can compute $I(X,Y) = H(X) - H(X|Y)$ for each video.  This tells us the information gain about $X$ (won/lost) that we get by specifying $Y$ (right/left).  Sometimes the *relative information gain* $I(X,Y)/H(X)$\n",
      "is more meaningful.  This is the fraction of the total information about $X$ that we get from $Y$.\n",
      "\n",
      "So why didn't we immediately convert to probabilities by dividing each table by its sum?  After all, we only need the joint distribution to compute mutual information, right?  True, but there's more to the story.  We need to evaluate the *significance* of each table.  Tables with a small number of counts will yield unreliable distributions due to random effects.  So how many votes do we need before we can place very much stock in the sample mutual information?  We're interested in the two columns of our table, because they are the conditional distributions on won/lost given right or left.  Just as in our first computation of bias, the bias for each column can be modeled as $b = S/N$, where S is the sum of $N$ i.i.d. random variables $X_i = \\pm 1$  each of mean $b$, so that $Pr(X_i = 1)$ is $p = (1+b)/2$.  Then the std. deviation of S is $\\sqrt{(1-b^2)N}$ and we get \"error bars\" for $b$ of size roughly $\\sqrt{(1-b^2)/N}$.  So it looks like one decimal place accuracy corresponds to something like 100 votes (in each column).  To keep things simple, let's make a list of the \"popular\" videos that got at least 200 votes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "popular = [[id,cont_tbl[id]] for id in ids if sum(cont_tbl[id]) >= 200]\n",
      "tot_pop = .5*sum([popular[i][1] for i in range (0,len(popular))])\n",
      "print \"there are \",len(popular),\" popular videos and they got a total of \",\\\n",
      "tot_pop,\" out of \",len(data), \" votes = \",tot_pop*100/len(data),\"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-8033dd5a16a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpopular\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcont_tbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_tbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtot_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"there are \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" popular videos and they got a total of \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtot_pop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" out of \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" votes = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtot_pop\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow! 7% of the videos got 78% of the votes! Now we can compute the mutual information for each of the \"popular\" videos.  But before we do, this is a good time to introduce another very classical independence test.\n",
      "## The Contingency Chi-Square Test\n",
      "Given a set of observed counts $O_1,O_2,\\dots,O_n$ and expected values for these counts $E_1,E_2,\\dots,E_n$ we compute \n",
      "$$\n",
      "\\chi^2 := \\sum_{i=1}^n\\frac{(O_i - E_i)^2}{E_i}.\n",
      "$$\n",
      "The idea is that, analagous to what we've already seen above, each term $(O_i-E_i)/\\sqrt{E_i}$ should be approximately normally distributed with mean 0 and variance 1.  So $\\chi^2$ is a sum of squares of (approximately) normal random variables.  But there may be some constraints on these variables.  For example, if the sum of all the counts is always 100, say, then there are really only $n-1$ \"degrees of freedom\".  Without dwelling on this sometimes tricky point, the \"degrees of freedom\" in a chi-square statistic (often denoted $\\nu$) is important.  The reason is that the tail probability function $t(x,\\nu) = Pr(\\chi^2 \\ge x)$,\n",
      "often called the *p-value*, can be computed.  So we can quantify how surprising (unexpected) a given set of counts is. For example, $1/t(x,\\nu)$ is often interpreted as the number of experiments we would expect to run if the $E_i$ were \"correct\" (i.e. if the so-called null hypothesis were true)  before seeing a $\\chi^2$ statistic as big or bigger than the one we got.\n",
      "\n",
      "For a contingency table we want to know whether the two sample marginal distributions $p$ and $q$ are independent, so the expected values are those that would arise if they were, namely $E_{ij} = p_iq_jN$. The number of degrees of freedom is a bit tricky, but in a $2\\times{2}$ table with given row and column sums, there is only one degree of freedom, because knowing any one count determines all the others.  So let's run the test on the popular videos.  In fact, let's append a column of chi-square p-values to the popular table, and save all the tables for which the chi-square p-value is less than 1/18,474.  Why this threshold?  Because you would expect to see less than 1 such table in a list of 18,474 tables generated under the null hypothesis that the marginals are independent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats\n",
      "def contingency_chisq(a):\n",
      "    #print a\n",
      "    #return [0,0]\n",
      "    p = a[0][0] + a[0][1]\n",
      "    q = a[0][0] + a[1][0]\n",
      "    n = sum(a)\n",
      "    obs = np.array([a[0][0],a[0][1],a[1][0],a[1][1]])\n",
      "    exp = np.array([p*q/n,p*(n-q)/n,(n-p)*q/n,(n-p)*(n-q)/n])\n",
      "#    print \"observed: \", obs\n",
      "#    print \"expected: \", exp\n",
      "    return scipy.stats.chisquare(obs,exp,1)\n",
      "#print popular[0:2]\n",
      "#print \"x[1]: \",[x[1] for x in popular[0:2]]\n",
      "popular1 = sorted([[contingency_chisq(x[1])[1],x[0],x[1]] for x in popular], key=my_key)\n",
      "hits = [x for x in popular1 if x[0] < 1.0/18474]\n",
      "print \"there were \",len(hits),\" significant chi-square hits: \", hits\n",
      "\n",
      "#print \"top ten chisq pvals: \", chisq[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'popular' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-71d9cae5d460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print popular[0:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print \"x[1]: \",[x[1] for x in popular[0:2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpopular1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontingency_chisq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopular\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopular1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m18474\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"there were \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" significant chi-square hits: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'popular' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's compute the mutual information for the significant chi-square hits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mutual_info(d): # compute mutual info for 2x2 distr. d\n",
      "    p = d[0,0] + d[0,1]\n",
      "    q = d[0,0] + d[1,0]\n",
      "    \n",
      "    try:\n",
      "        mut_info = d[0,0]*log(d[0,0]/(p*q))\\\n",
      "            + d[0,1]*log(d[0,1]/(p*(1-q)))\\\n",
      "            + d[1,0]*log(d[1,0]/((1-p)*q))\\\n",
      "            + d[1,1]*log(d[1,1]/((1-p)*(1-q)))\n",
      "    except:\n",
      "        print d\n",
      "        return [1,0,0]\n",
      "    info = np.zeros(3)\n",
      "    info[0] = mut_info/log(2) #mutual info in bits\n",
      "    info[1] = -mut_info/(p*log(p)+(1-p)*log(1-p))# relative information gains\n",
      "    info[2] = -mut_info/(q*log(q)+(1-q)*log(1-q))\n",
      "    return info\n",
      "\n",
      "minfo = [mutual_info(x[2]/sum(x[2]))[1] for x in hits] # relative info gain in X given Y\n",
      "print \"relative info gain for chi-square hits: \", minfo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "relative info gain for chi-square hits:  [0.00074163044442668866, 0.0010130032114768063, 0.00060949725997176663, 0.00048236521464107462, 0.00083416008115283827, 0.00031451661011852318, 0.02993623874223315, 0.00046692452976976757]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So it doesn't look like knowing $Y$ really helps with $X$ very much. But we have clearly illustrated the difference between $\\chi^2$ and mutual information.  The former only tells you that there's a very small chance that the marginals are really independent.  But it says nothing about how dependent they are.  The latter tells you how dependent the *sample* marginals are for a particular table, but says nothing about the chance of that happening at random if the \"real\" marginals were in fact independent. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Probability Models\n",
      "We're going to look at pairs of videos and the metadata associated with them and try to predict which one will be funnier.  So you might think that we want an algorithm that inputs the metadata and outputs 0 if it thinks the left-hand video is funnier or 1 if it thinks the right-hand video is funnier.  But since this is a statistics class, we will instead estimate the *probability*, $p$, that the right-hand one is funnier.  Besides, this turns out to work better in most cases.  One obvious reason for this is that there's clearly a big difference between the outputs $p = .51$ and $p = .95$, even though both of these say that the right-hand video is funnier.  But there are many other not-so-obvious reasons why probability models are better than deterministic models for this sort of problem, and hopefully we'll see some of them as we go along. The model we are going to use is called *logistic regression*.  The python package statsmodels.api has a good implementation, and there is an excellent example of it [here](http://blog.yhathq.com/posts/logistic-regression-and-python.html).  This is a highly instructive ipython notebook which not only shows logistic regression in action but also demonstrates some cool features of pandas.  Please read it.\n",
      "Now, if this were an undergraduate course, we could just use the python package without worrying about how it works.\n",
      "But this is a graduate course, and you are supposed to be Masters of Statistics!  So we're going to spend some time learning what's really going on.\n",
      "\n",
      "So, what exactly is a *probability model*?  Basically, it's just a real-valued non-negative function $f(y|x;\\theta)$,\n",
      "where $y$ is the quantity we're trying to predict,  $x$ is a vector of $n$ real numbers which we will call *predictors*, and $\\theta$ is a vector of model parameters.  The value of $f$ is the conditional likelihood of $y$ given $x$ and $\\theta$. \n",
      "There is no standard terminology -- in machine learning $x$ is often called the \"context\" and $y$ the \"goal\". Typically, we are given a data set $(y_i,x_i),~i = 1,2,\\dots,N$ called the \"training set\" which we use to try to figure out the best set of parameters $\\theta$. This is called *training the model*.\n",
      "\n",
      "Assuming that the pairs $(y_i,x_i)$ are samples of independent random variables, the likelihood of the entire training set is the product\n",
      "$$\n",
      "L = \\prod_{i=1}^N f(y_i|x_i;\\theta),\n",
      "$$\n",
      "and the \"best\" value of theta is usually taken to mean the value that maximizes $L$. So the training process is called *Maximum Likelihood Estimation* or MLE for short. It's usually easier to maximize \n",
      "$$\n",
      "L^* = \\log(L) = \\sum_{i=1}^N\\log(f(y_i|x_i;\\theta)\n",
      "$$\n",
      "and the resulting value of $\\theta$ is the same in either case (why?).\n",
      "\n",
      "Once we have trained the model and we have values for $\\theta$, we can use it to compute the *predictive distribution* which is the conditional probability distribution of $y$ given a \"test\" predictor vector $x_j$. This distribution is really our prediction for $y$ given $x_j$, but sometimes we need a value, and not just a distribution on all possible values.  A standard choice is the mean of the distribution, but we might also take the median.  One advantage of having a distribution is that we automatically have error estimates.  \n",
      "\n",
      "Logistic Regression is a particular example of MLE, but before we get to this, it's instructive to begin with the canonical example.\n",
      "\n",
      "##Linear Regression\n",
      "The simplest possible relationship between $y$ and $x$ would be linear: \n",
      "\n",
      "$$\n",
      "y_i = x_i\\cdot\\theta, \\quad 1\\le i\\le N,\n",
      "$$\n",
      "\n",
      "for some magic vector $\\theta$. Now in real life, such an $\\theta$ does not exist, but we might pretend that there is some additive measurement noise $\\epsilon_i$ which is responsible for this.  So perhaps a magic $\\theta$ really does exist, but it is hidden in noise:\n",
      "\n",
      "$$\n",
      "y_i = x_i\\cdot\\theta + \\epsilon_i,\\quad 1\\le i\\le N.\n",
      "$$\n",
      "\n",
      "And if we further pretend that the $\\epsilon_i$ are independent and normally distributed  with mean zero and known variance $\\sigma_i^2$, we get a probability model:\n",
      "\n",
      "$$\n",
      "f(y_i|x_i;\\theta) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}}e^{-\\epsilon_i^2/2\\sigma_i^2} = \\frac{1}{\\sigma_i\\sqrt{2\\pi}}\\exp[-\\frac{1}{2\\sigma_i^2}(y_i-x_i\\cdot\\theta)^2].\n",
      "$$\n",
      "\n",
      "This is not just a likelihood function, it's a probability density because I included the normalizing factor $1/\\sigma\\sqrt{2\\pi}$ so that\n",
      "\n",
      "$$\n",
      "\\int f(y|,x_i;\\theta)dy = 1.\n",
      "$$\n",
      "\n",
      "But the log-likelihood function is\n",
      "\n",
      "$$\n",
      "L^* = \\sum_{i = 1}^N -\\log(\\sigma_i\\sqrt{2\\pi}) - \\frac{1}{2}\\sum_{i=1}^N\\frac{(y_i-x_i\\cdot\\theta)^2}{\\sigma_i^2}.\n",
      "$$\n",
      "\n",
      "so the normalizing factor doesn't matter since the first sum is independent of $\\theta$.   The maximum likelihood problem here is just the classical weighted least-squares problem: choose $\\theta$ to minimize (because of the minus sign) the right-hand sum of squares.  Hopefully you've seen this before. We get linear equations in the $n$ parameters $\\theta = \\theta_1,\\dots,\\theta_n$ by setting the partial derivatives of the log-likelihood to zero:\n",
      "\n",
      "$$\n",
      "\\frac{\\partial{L^*}}{\\partial{\\theta_j}} = 0 \\quad 1\\le j \\le n.\n",
      "$$\n",
      "\n",
      "These equations can be written in matrix notation as:\n",
      "\n",
      "$$\n",
      "X^tX\\theta = X^tY,\n",
      "$$\n",
      "\n",
      "where $X$ is the $N\\times{n}$ matrix whose $i^{th}$ row is $x_i/\\sigma_i$ and $Y$ is the $N\\times{1}$ vector whose $i^{th}$ component is $y_i/\\sigma_i$ as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "0 = \\frac{\\partial{L^*}}{\\partial{\\theta_j}} = -\\sum_{i=1}^Nx_{ij}(y_i-\\sum_{k=1}^nx_{ik}\\theta_k) = -\\sum_{i=1}^Nx_{ij}y_i + \\sum_{k=1}^n\\sum_{i=1}^Nx_{ij}x_{ik}\\theta_k\\quad 1\\le j\\le n,\n",
      "$$\n",
      "\n",
      "which in matrix notation is exactly the above equation.  These equations are called the *normal equations*.   \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The solution is\n",
      "\n",
      "$$\n",
      "\\hat{\\theta} = (X^tX)^{-1}X^tY.\n",
      "$$\n",
      "\n",
      "Now this is all very well, but how often are we going to find a dataset $(y,x)$ where $y$ differs from a linear function of \n",
      "$x$ by a normally distributed *residual* $\\epsilon$?  The answer is not very often, but even when there's some reason to\n",
      "believe that the noise is \"white\", it is usually not independent across samples, but instead comes in bursts. That's why I said we had to pretend when we wrote down the model.  This is an instance of Goldschmidt's second rule of modeling: **All models lie!**  We're always making assumptions that we know are not correct.  We just hope that the lies don't matter too much.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simulation\n",
      "Another important advantage of probability models is that they can be used, more or less as is, to produce simulated data.  One simply chooses a set of parameters $\\theta$ and generates predictor vectors by whatever method seems appropriate.  The model then gives a probability distribution for the corresponding $y$-value, and we can sample it to get an actual $y$-value. Note that in general we do have to normalize here by dividing by the sum (or integral in the continuous case) of the likelihood over all values of $y$. \n",
      "\n",
      "I have simulated some data for linear regression with $\\sigma_i = 1$ for all $i$, and pushed a file of training data to the repository. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate simulated linear data\n",
      "import numpy as np\n",
      "import numpy.random as random\n",
      "\n",
      "def linear_sim(fname,theta,seed,npts):\n",
      "    data = open(fname,\"w\")\n",
      "    random.seed(seed)\n",
      "    n = len(theta) # no. of parameters\n",
      "    for i in range(0,npts):\n",
      "        y = random.normal() # add noise\n",
      "        for j in range(0,n):\n",
      "            x_j = random.normal()\n",
      "            y += x_j*theta[j]\n",
      "            data.write(str(x_j)+\",\")\n",
      "        data.write(str(y)+'\\n')\n",
      "    data.close()\n",
      "\n",
      "theta = [1,-1,2,-2,3]\n",
      "linear_sim(\"simulated_data.train\",theta,1234,1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can go ahead and train up a linear model as above, obtaining the MLE values of the parameters $\\theta$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# solve normal equations for theta_hat\n",
      "from numpy import matrix\n",
      "\n",
      "XY = matrix(np.genfromtxt(\"simulated_data.train\",delimiter=','))\n",
      "X = XY[:, 0:5]\n",
      "Y = XY[:, 5]\n",
      "theta_hat = (X.T * X).I * X.T * Y\n",
      "print \"theta_hat:\\n\",theta_hat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "theta_hat:\n",
        "[[ 0.94437015]\n",
        " [-1.01835304]\n",
        " [ 2.02131576]\n",
        " [-1.96810941]\n",
        " [ 2.98993613]]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "##Scoring\n",
      "So, how accurate is the linear model we trained above?  We see that the MLE value of $\\theta$ is \"pretty close\" to the actual value, so things look good. But the only reason we have the actual parameter values to compare against is because this is a simulation!  In real life, we don't  know the actual values, and in fact as pointed out earlier, there probably aren't any actual values.  Fortunately, we don't need \"actual\" parameter values to test the accuracy of the model.  Instead, we use another *test set* of data\n",
      "$(y_i,x_i)~1\\le i\\le N_1$.  One thing we could do would be to make a *point prediction*  $\\hat{y}_i$ for every test predictor $x_i$, such as the mean of the predictive distribution or perhaps the median.  Then we could use the sum of squares of the differences\n",
      "\n",
      "$$\n",
      "\\sum_{i=1}^{N_1}(\\hat{y}_i - y_i)^2\n",
      "$$\n",
      "\n",
      "as an accuracy measure.  But this kind of score doesn't test how confident we were about $\\hat{y}_i$. Fortunately, there is an information-theoretic score that really tests the quality of the predictive distribution.  Let $p(y|x_i;\\theta)$ be the normalized conditional likelihood function.  This is a probability distribution on $y$ in the discrete case and a probability density function of $y$ in the continuous case.  Then we compute \n",
      "\n",
      "$$\n",
      "\\hat{H}(Y|X) = -\\frac{1}{N_1}\\sum_{i=1}^{N_1}\\log{p(y_i|x_i;\\theta)}.\n",
      "$$\n",
      "\n",
      "How do we interpret $\\hat{H}~$?  Since we haven't really talked about differential entropy for continuous random variables, let's keep things simple and suppose there are only finitely many values of $y$ and $x$.  We can always approximate continuous variables by finite ones using small bins.  Now suppose the \"true\" conditional distribution of $y$ given $x$ is $q(y|x)$ and the \"true\" distribution of $x$ is\n",
      "$r(x)$.  Then for large $N_1$, we will find the pair $(y_i,x_i)$ in the test set with frequency approximately $q(y_i|x_i)r(x_i)$ so that\n",
      "\n",
      "$$\n",
      "\\hat{H} \\approx -\\sum_{x,y} q(y|x)r(x)\\log(p(y|x;\\theta) = -\\sum_xr(x)\\sum_yq(y|x)\\log(p(y|x;\\theta).\n",
      "$$\n",
      "\n",
      "If we had $q = p$, that is if $p$ really were the true conditional distribution, the right-hand sum would be the conditional entropy \n",
      "\n",
      "$$\n",
      "H(Y|X) = -\\sum_xr(x)\\sum_yq(y|x)\\log{q(y|x)}.\n",
      "$$\n",
      "\n",
      "Instead, it is called the (conditional) *cross-entropy* $H_q(Y|X)$.  More generally, for two probability distributions $p$ and $q$ (defined on the same set) the difference  \n",
      "\n",
      "$$\n",
      "\\sum_xp(x)\\log{p(x)} - \\sum_x p(x)\\log{q(x)} = \\sum_xp(x)\\log{(p(x)/q(x))}\n",
      "$$\n",
      "\n",
      "is called the *Kullback-Liebler* divergence of $q$ from $p$ and is always positive unless $p = q$. It measures how diffent $q$ is from $p$. We have already seen an example of this:  mutual information is just the KL-divergence of the product $q(X)r(Y)$ of the marginals of a joint distribution $p(X,Y)$ from $p(X,Y)$ itself.\n",
      "\n",
      "But getting back to our score $\\hat{H}$, since we don't know the true conditional distribution $q(y|x)$ here, this interpretation is entirely theoretical.  From a practical point of view, we can interpret $\\hat{H}$ as a compression factor.  It is the average number of bits per observation we would need to Huffman code the $y_i$ values.  We can often compare this directly with the number of bits needed to express the $y_i$ without knowing $x_i$.  For example, if $y$ were a binary variable, $\\hat{H}$ would be a fraction of a bit.  Things aren't as simple in the continuous case, and we won't go into the details here.  Nevertheless, let's generate a test set for the linear regression we ran above and score it twice:  once with the trained $\\hat{\\theta}$ model and once with the true $\\theta$ model:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import stats\n",
      "from numpy import matrix\n",
      "\n",
      "def ConEntro(X, Y, Theta, sd = 1):\n",
      "    mu = X * Theta\n",
      "    p = matrix(stats.norm.pdf(Y, loc = mu, scale = sd))\n",
      "    CE = -np.average(np.log(p))\n",
      "    return CE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate and score test data   \n",
      "linear_sim(\"simulated_data.test\",theta,1111,1000)\n",
      "XY = matrix(np.genfromtxt(\"simulated_data.test\",delimiter=','))\n",
      "X = XY[:, 0:5]\n",
      "Y = XY[:, 5]\n",
      "Score_true = ConEntro(X, Y, theta_true, 1)\n",
      "Score_est = ConEntro(X, Y, theta_hat, 1)\n",
      "print Score_true\n",
      "print Score_est\n",
      "print Score_true - Score_est"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.42205540491\n",
        "1.42413000746\n",
        "-0.00207460254679\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true model scores slightly better on the test set than the trained model, as we might expect.  But here's an experiment:  set the seed for the test set generation to 1234 so that the test set is identical to the training set. What happens?  Both scores get better,\n",
      "which we would expect, but now the trained model does better than the true model.  Should we have expected this?  Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trained model perform better because theta_hat is selected to maximize the log likelihood function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression\n",
      "In linear regression, the dependent variable $y$ is continuous, but for the comedy comparison problem we need a model with a binary dependent variable. Logistic regression is perhaps the simplest example of this. Our model is going to look at the metadata for a pair of videos and predict which one is funnier . Let $y = 1$ if the right-hand video is funnier, and $y = -1$ if the left-hand video is funnier.  Our predictors will be a vector $x$ of $n$ real numbers obtained from the metadata.  The conditional probability of $y$ given $x$ is\n",
      "\n",
      "$$\n",
      "p(y|x;\\theta) = \\frac{1}{1+e^{-yx\\cdot\\theta}},\n",
      "$$\n",
      "\n",
      "where $\\theta$ is a vector of $n$ parameters, just as in linear regression.  Since the exponential maps the real line to the positive real line, it's easy to see that $0\\le p(y|x;\\theta) < 1$. In fact we have\n",
      "$p(1|x;\\theta) + p(-1|x;\\theta) = 1$ as the following calculation shows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\frac{1}{1+e^{-x\\theta}} + \\frac{1}{1 + e^{x\\theta}} = \\frac{(1+e^{x\\theta})+(1+e^{-x\\theta})}{(1+ e^{x\\theta})(1+e^{-x\\theta})} = 1$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So $p$ is already a probability distribution on $y$, which is why I called it $p$. Let's calculate the odds of $y = 1$ over $y = -1$:\n",
      "\n",
      "$$\n",
      "\\frac{p}{1-p} = \\frac{1}{1+e^{-x\\cdot\\theta}}\\cdot\\frac{1+e^{-x\\cdot\\theta}}{e^{-x\\cdot\\theta}} = e^{x\\cdot\\theta},\n",
      "$$\n",
      "\n",
      "which means that the log-odds are just $x\\cdot\\theta$, a linear function of the predictors $x$ with coefficients $\\theta$. So if we could only observe the log-odds directly, we would be in the linear regression case.  Instead, we only get to see the\n",
      "value of $y$, which is one bit of information per observation.  This means that logistic regression is inherently much weaker than linear regression, in the sense that more data is required to achieve the same degree of accuracy in the coefficients.\n",
      "\n",
      "Here is a plot of the logistic curve:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "def f(x):\n",
      "    return 1/(1+math.exp(-x)) \n",
      "plt.plot([x for x in np.arange(-10,10,.1)],[f(x) for x in np.arange(-10,10,.1)])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that almost all the action occurs in the interval $[-5,5]$, which means that if we have a predictor which varies over a significantly wider interval, we're going to have trouble capturing its effect.  We'll return to this issue later.  Let's turn now to the MLE problem.\n",
      "\n",
      "The logistic function is mathematically simpler than the corresponding normal cumulative distribution function, but paradoxically, the MLE problem is more difficult. For a dataset $(y_i,x_i)~1\\le i\\le N$, the log-likelihood function is \n",
      "\n",
      "$$\n",
      "L^* = -\\sum_{i=1}^N\\log(1+e^{-y_ix_i\\cdot\\theta}),\n",
      "$$\n",
      "\n",
      "and maximizing this $L^*$ with respect to $\\theta$ is unfortunately a *non-linear* optimization problem.  It turns out that there is a general non-linear optimization technique known as Newton's method which works well here, and if we have time, we'll try to program it and compare with the statsmodel library function.  But for now, let's simulate some data, solve with the library routine, and score our answers, just like we did above for linear regression.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels.api as sm\n",
      "from numpy.random import binomial \n",
      "\n",
      "def simLogit(fname, size, theta, seed):\n",
      "    data = open(fname, 'w')\n",
      "    random.seed(seed)\n",
      "    n = len(theta)\n",
      "    for i in range(0, size):\n",
      "        #y = random.normal() # add noise\n",
      "        s = 0\n",
      "        for j in range(0, n):\n",
      "            x_j = random.normal()\n",
      "            s += x_j*theta[j]\n",
      "            data.write(str(x_j)+\",\")\n",
      "        p = 1/(1 + np.exp(-s))\n",
      "        y = binomial(1, p)\n",
      "        data.write(str(y)+'\\n')\n",
      "    data.close()\n",
      "            \n",
      "theta = [1, -1, 2, -2, 3]\n",
      "simLogit(\"simulated_logit_data.train\", 1000, theta, 1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XY = matrix(np.genfromtxt(\"simulated_logit_data.train\",delimiter=','))\n",
      "X = XY[:, 0:5]\n",
      "Y = XY[:, 5]\n",
      "result = statsmodels.api.Logit(Y, X).fit()\n",
      "log_theta_hat = result.params\n",
      "#print result.summary()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 257.639032\n",
        "         Iterations 8\n",
        "0.475448727808\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_hat\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "matrix([[ 0.94437015],\n",
        "        [-1.01835304],\n",
        "        [ 2.02131576],\n",
        "        [-1.96810941],\n",
        "        [ 2.98993613]])"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use the Logit class of statsmodels.api in the same way that the web article linked in the first paragraph of this section did \n",
      "# Generate test data\n",
      "\n",
      "simLogit(\"simulated_logit_data.test\", 1000, theta, 2222)\n",
      "XY = matrix(np.genfromtxt(\"simulated_logit_data.test\",delimiter=','))\n",
      "X = XY[:, 0:5]\n",
      "Y = XY[:, 5]\n",
      "temp_est = X * theta_hat\n",
      "log_score_est = 0\n",
      "for i in range(0, len(temp_est)):\n",
      "    log_score_est += math.log(1 + math.exp(-Y[i]*temp_est[i]))\n",
      "log_score_est = log_score_est/len(temp_est)\n",
      "print theta_hat.T\n",
      "print log_score_est\n",
      "\n",
      "temp_true = X * matrix(theta).T\n",
      "log_score_true = 0\n",
      "for i in range(0, len(temp_true)):\n",
      "    log_score_true += math.log(1 + math.exp(-Y[i]*temp_true[i]))\n",
      "log_score_true = log_score_true/len(temp_true)\n",
      "print matrix(theta)\n",
      "print log_score_true\n",
      "\n",
      "print log_score_true - log_score_est"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.94437015 -1.01835304  2.02131576 -1.96810941  2.98993613]]\n",
        "0.480904041094\n",
        "[[ 1 -1  2 -2  3]]\n",
        "0.480610807904\n",
        "-0.000293233189643\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hmm, a nice-looking display, but the coefficients are all about twice as big as they should be!  Note:  I added some noise to the exponent after computing $x_i\\cdot\\theta$, because without it I got a \"perfect separation\" error which seemed to say that the data was too accurate!  Let's compare this \"doubled\" model with the true model on a 1000-point test set:  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "garble probability  0 : true model score:  0.28115293815 bits. trained model score:  0.256997094553 bits. delta:  0.0241558435963\n",
        "garble probability "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1 : true model score:  0.762280453885 bits. trained model score:  1.19406344376 bits. delta:  -0.43178298987\n",
        "garble probability "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.2 : true model score:  1.27810753281 bits. trained model score:  2.21128955828 bits. delta:  -0.933182025465\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The trained model outscores the true model on the test set.  But now let's add some noise.  Wait! How do we add noise to a binary variable?  We randomly garble some of the $y$ bits.  That is, with some probability $g$, we switch the response to the opposite response.\n",
      "Notice how the (over)trained model scores now.  This is a characteristic feature of over-training.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    }
   ],
   "metadata": {}
  }
 ]
}